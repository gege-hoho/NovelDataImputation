{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0RQ0wjd696j",
        "outputId": "423cccc8-ac15-4633-f6e1-3f5be808791b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/imputation/* ."
      ],
      "metadata": {
        "id": "ZsmpVE9X68o9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AKL1BqLpOrry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "INPUT_SIZE = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "B5ezaSKiu0_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') \n",
        "    return device\n",
        "device = get_device()\n",
        "print(device)"
      ],
      "metadata": {
        "id": "qPByV49cUi4-",
        "outputId": "2d85c49f-9c51-4eae-a451-9302bdf8e9d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(input_size)).to(device)\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        m = m.to(device)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(output_size)).to(device)\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size).to(device)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "class RITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE).to(device)\n",
        "\n",
        "        self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "        self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "        self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE).to(device)\n",
        "        self.feat_reg = FeatureRegression(INPUT_SIZE).to(device)\n",
        "\n",
        "        self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE).to(device)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.25).to(device)\n",
        "        self.out = nn.Linear(RNN_HID_SIZE, 1).to(device)\n",
        "\n",
        "    def forward(self, data, direct):\n",
        "        values = data[direct]['values'].to(device)\n",
        "        masks = data[direct]['masks'].to(device)\n",
        "        deltas = data[direct]['deltas'].to(device)\n",
        "\n",
        "        evals = data[direct]['evals'].to(device)\n",
        "        eval_masks = data[direct]['eval_masks'].to(device)\n",
        "\n",
        "        h = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "        c = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "          \n",
        "\n",
        "        x_loss = 0.0\n",
        "        y_loss = 0.0\n",
        "\n",
        "        imputations = []\n",
        "\n",
        "        for t in range(SEQ_LEN):\n",
        "            x = values[:, t, :]\n",
        "            m = masks[:, t, :]\n",
        "            d = deltas[:, t, :]\n",
        "\n",
        "            gamma_h = self.temp_decay_h(d)\n",
        "            gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "            h = h * gamma_h\n",
        "\n",
        "            x_h = self.hist_reg(h)\n",
        "            x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            x_c =  m * x +  (1 - m) * x_h\n",
        "\n",
        "            z_h = self.feat_reg(x_c)\n",
        "            x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))\n",
        "\n",
        "            c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "            x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "            inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "            h, c = self.rnn_cell(inputs, (h, c))\n",
        "\n",
        "            imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "        imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "        return {'loss': x_loss / SEQ_LEN,\n",
        "                'imputations': imputations,\n",
        "                'evals': evals, \n",
        "                'eval_masks': eval_masks}\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data, direct = 'forward')\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data):\n",
        "        ret_f = self.rits_f(data, 'forward')\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward'))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_normalizations(file):\n",
        "  content = open(file,'rb')\n",
        "  ret = pickle.load(content)\n",
        "  content.close()\n",
        "  return ret"
      ],
      "metadata": {
        "id": "rxFsveZ7EBzP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "class MySet2(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(MySet2, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        tens = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "        return_dict[dict_key] = tens \n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn2(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "      backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "abyIB8rswJMB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "A-nIWo1KUc2R"
      },
      "outputs": [],
      "source": [
        "indexes= {'meal':0,'calories':1,'carbs':2,'fat':3,'protein':4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 500\n",
        "use_norm = True\n",
        "path_train = './brits_train.pickle'\n",
        "path_test = './brits_test.pickle'\n",
        "if not use_norm:\n",
        "  path_train = './brits_train_nonnorm.pickle'\n",
        "  path_test = './brits_test_nonnorm.pickle'\n",
        "normalizations = load_normalizations('brits_normalization.pickle')\n",
        "\n",
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "train_set = MySet2(path_train)\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = batch_size,shuffle = True,pin_memory = True, collate_fn = collate_fn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "RxB3iYgjRfGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a1e10a-c445-4bc7-e921-577eaf3aac4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Progress epoch 0, 100.00%, average loss 46.651329040527344\n",
            " Progress epoch 1, 100.00%, average loss 46.3891716003418\n",
            " Progress epoch 2, 100.00%, average loss 46.13164520263672\n",
            " Progress epoch 3, 100.00%, average loss 45.8769416809082\n",
            " Progress epoch 4, 100.00%, average loss 45.632198333740234\n",
            " Progress epoch 5, 100.00%, average loss 45.39121627807617\n",
            " Progress epoch 6, 100.00%, average loss 45.15153884887695\n",
            " Progress epoch 7, 100.00%, average loss 44.91617202758789\n",
            " Progress epoch 8, 100.00%, average loss 44.68472671508789\n",
            " Progress epoch 9, 100.00%, average loss 44.455814361572266\n",
            " Progress epoch 10, 100.00%, average loss 44.22711944580078\n",
            " Progress epoch 11, 100.00%, average loss 43.999755859375\n",
            " Progress epoch 12, 100.00%, average loss 43.773197174072266\n",
            " Progress epoch 13, 100.00%, average loss 43.549922943115234\n",
            " Progress epoch 14, 100.00%, average loss 43.32828903198242\n",
            " Progress epoch 15, 100.00%, average loss 43.10932540893555\n",
            " Progress epoch 16, 100.00%, average loss 42.89332962036133\n",
            " Progress epoch 17, 100.00%, average loss 42.68104934692383\n",
            " Progress epoch 18, 100.00%, average loss 42.470787048339844\n",
            " Progress epoch 19, 100.00%, average loss 42.262638092041016\n",
            " Progress epoch 20, 100.00%, average loss 42.05363464355469\n",
            " Progress epoch 21, 100.00%, average loss 41.84477615356445\n",
            " Progress epoch 22, 100.00%, average loss 41.63837432861328\n",
            " Progress epoch 23, 100.00%, average loss 41.43295669555664\n",
            " Progress epoch 24, 100.00%, average loss 41.229801177978516\n",
            " Progress epoch 25, 100.00%, average loss 41.03425979614258\n",
            " Progress epoch 26, 100.00%, average loss 40.84475326538086\n",
            " Progress epoch 27, 100.00%, average loss 40.65896224975586\n",
            " Progress epoch 28, 100.00%, average loss 40.477622985839844\n",
            " Progress epoch 29, 100.00%, average loss 40.30101013183594\n",
            " Progress epoch 30, 100.00%, average loss 40.130104064941406\n",
            " Progress epoch 31, 100.00%, average loss 39.96505355834961\n",
            " Progress epoch 32, 100.00%, average loss 39.802642822265625\n",
            " Progress epoch 33, 100.00%, average loss 39.641788482666016\n",
            " Progress epoch 34, 100.00%, average loss 39.482486724853516\n",
            " Progress epoch 35, 100.00%, average loss 39.3240852355957\n",
            " Progress epoch 36, 100.00%, average loss 39.16719055175781\n",
            " Progress epoch 37, 100.00%, average loss 39.01063919067383\n",
            " Progress epoch 38, 100.00%, average loss 38.85580062866211\n",
            " Progress epoch 39, 100.00%, average loss 38.703914642333984\n",
            " Progress epoch 40, 100.00%, average loss 38.55459976196289\n",
            " Progress epoch 41, 100.00%, average loss 38.408729553222656\n",
            " Progress epoch 42, 100.00%, average loss 38.26820755004883\n",
            " Progress epoch 43, 100.00%, average loss 38.12989044189453\n",
            " Progress epoch 44, 100.00%, average loss 37.994781494140625\n",
            " Progress epoch 45, 100.00%, average loss 37.86812973022461\n",
            " Progress epoch 46, 100.00%, average loss 37.74308395385742\n",
            " Progress epoch 47, 100.00%, average loss 37.619972229003906\n",
            " Progress epoch 48, 100.00%, average loss 37.499874114990234\n",
            " Progress epoch 49, 100.00%, average loss 37.38410949707031\n",
            " Progress epoch 50, 100.00%, average loss 37.27362060546875\n",
            " Progress epoch 51, 100.00%, average loss 37.16538619995117\n",
            " Progress epoch 52, 100.00%, average loss 37.057708740234375\n",
            " Progress epoch 53, 100.00%, average loss 36.949974060058594\n",
            " Progress epoch 54, 100.00%, average loss 36.841915130615234\n",
            " Progress epoch 55, 100.00%, average loss 36.73341369628906\n",
            " Progress epoch 56, 100.00%, average loss 36.625343322753906\n",
            " Progress epoch 57, 100.00%, average loss 36.51755142211914\n",
            " Progress epoch 58, 100.00%, average loss 36.40946960449219\n",
            " Progress epoch 59, 100.00%, average loss 36.302284240722656\n",
            " Progress epoch 60, 100.00%, average loss 36.19658279418945\n",
            " Progress epoch 61, 100.00%, average loss 36.097259521484375\n",
            " Progress epoch 62, 100.00%, average loss 36.00217819213867\n",
            " Progress epoch 63, 100.00%, average loss 35.908180236816406\n",
            " Progress epoch 64, 100.00%, average loss 35.814334869384766\n",
            " Progress epoch 65, 100.00%, average loss 35.72119903564453\n",
            " Progress epoch 66, 100.00%, average loss 35.62885284423828\n",
            " Progress epoch 67, 100.00%, average loss 35.5369873046875\n",
            " Progress epoch 68, 100.00%, average loss 35.444087982177734\n",
            " Progress epoch 69, 100.00%, average loss 35.350337982177734\n",
            " Progress epoch 70, 100.00%, average loss 35.25651550292969\n",
            " Progress epoch 71, 100.00%, average loss 35.163150787353516\n",
            " Progress epoch 72, 100.00%, average loss 35.07022476196289\n",
            " Progress epoch 73, 100.00%, average loss 34.97707748413086\n",
            " Progress epoch 74, 100.00%, average loss 34.883811950683594\n",
            " Progress epoch 75, 100.00%, average loss 34.79020690917969\n",
            " Progress epoch 76, 100.00%, average loss 34.69657516479492\n",
            " Progress epoch 77, 100.00%, average loss 34.60279083251953\n",
            " Progress epoch 78, 100.00%, average loss 34.50869369506836\n",
            " Progress epoch 79, 100.00%, average loss 34.41453552246094\n",
            " Progress epoch 80, 100.00%, average loss 34.320499420166016\n",
            " Progress epoch 81, 100.00%, average loss 34.22640609741211\n",
            " Progress epoch 82, 100.00%, average loss 34.13229751586914\n",
            " Progress epoch 83, 100.00%, average loss 34.040191650390625\n",
            " Progress epoch 84, 100.00%, average loss 33.947914123535156\n",
            " Progress epoch 85, 100.00%, average loss 33.854896545410156\n",
            " Progress epoch 86, 100.00%, average loss 33.761348724365234\n",
            " Progress epoch 87, 100.00%, average loss 33.66769027709961\n",
            " Progress epoch 88, 100.00%, average loss 33.57366180419922\n",
            " Progress epoch 89, 100.00%, average loss 33.480262756347656\n",
            " Progress epoch 90, 100.00%, average loss 33.38822937011719\n",
            " Progress epoch 91, 100.00%, average loss 33.295631408691406\n",
            " Progress epoch 92, 100.00%, average loss 33.20262145996094\n",
            " Progress epoch 93, 100.00%, average loss 33.10997772216797\n",
            " Progress epoch 94, 100.00%, average loss 33.01691436767578\n",
            " Progress epoch 95, 100.00%, average loss 32.92387008666992\n",
            " Progress epoch 96, 100.00%, average loss 32.83198928833008\n",
            " Progress epoch 97, 100.00%, average loss 32.74277877807617\n",
            " Progress epoch 98, 100.00%, average loss 32.65483093261719\n",
            " Progress epoch 99, 100.00%, average loss 32.56686019897461\n",
            "cpu 26.220885276794434\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  run_loss = 0.0\n",
        "\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "\n",
        "    run_loss += ret['loss'].item()\n",
        "\n",
        "    print('\\r Progress epoch {}, {:.2f}%, average loss {}'.format(epoch, (idx + 1) * 100.0 / len(train_iter), run_loss / (idx + 1.0)))\n",
        "\n",
        "print(f\"{device} {time.time()-t0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "d5UUnb13QYib"
      },
      "outputs": [],
      "source": [
        "test_set = MySet2(path_test)\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = batch_size,shuffle = False,pin_memory = True, collate_fn = collate_fn2)\n",
        "model.eval()\n",
        "for idx, data in enumerate(test_iter):\n",
        "  ret = model.run_on_batch(data, None)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "ogmrBN84SqF2"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]"
      ],
      "metadata": {
        "id": "uf2tsKatEPel"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skdVIxrTa9Qf",
        "outputId": "6d0b34d6-2a68-40dc-f7ab-7d0b5d91c28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN   221.11209106445312\n",
            "MEAN  285.0473937988281\n",
            "MEANP 296.0644836425781\n"
          ]
        }
      ],
      "source": [
        "def print_impu_real(ret,index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1)#get all rows that are missing to compare to ground truth\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  px = pd.DataFrame()\n",
        "  px.insert(0,\"Real\",real.numpy())\n",
        "  px.insert(0,\"Imputation\",impu.detach().numpy())\n",
        "  return px\n",
        "\n",
        "def calc_error_on_row(ret, index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1)#get all rows that are missing to compare to ground truth\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  return torch.abs(impu-real).mean()\n",
        "\n",
        "def calculate_personal_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean()\n",
        "\n",
        "def calc_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  filter = (forward['eval_masks'][:,:,index]==1)#get all rows that are missing\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  #mean_erro = torch.abs(real-z).mean()\n",
        "  mean_erro = torch.abs(real-z).mean()\n",
        "  return mean_erro\n",
        "\n",
        "index_name = 'calories'\n",
        "mean_erro = calc_mean_error(data,index_name)\n",
        "personal_mean_erro = calculate_personal_mean_error(data,index_name)\n",
        "rnn_erro = calc_error_on_row(ret,index_name)\n",
        "\n",
        "print(f\"RNN   {rnn_erro}\")\n",
        "print(f\"MEAN  {mean_erro}\")\n",
        "print(f\"MEANP {personal_mean_erro}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_impu_real(ret,index_name)"
      ],
      "metadata": {
        "id": "zGDNiRFQLEnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizations"
      ],
      "metadata": {
        "id": "X6Vi8cojJJKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use_norm = True\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "  \n",
        "\n",
        "in_file =  open(path_test,'rb')\n",
        "recs = pickle.load(in_file)\n",
        "in_file.close()\n",
        "#recs = [json.loads(x) for x in content]\n",
        "recs = [x['forward'] for x in recs]\n",
        "#recs = [[(y['values'][indexes['calories']] - normalizations['mean']['calories'] )/ normalizations['std']['calories'] for y in x]for x in recs]\n",
        "#recs = flatten(recs)\n",
        "#recs = [revert_norm(x,'calories') for x in recs]\n",
        "recs = [[revert_norm(y['values'][indexes['calories']],'calories')for y in x]for x in recs]\n",
        "recs = flatten(recs)\n",
        "recs = np.array(recs)\n",
        "recs.mean()"
      ],
      "metadata": {
        "id": "vk50FGc292jD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}