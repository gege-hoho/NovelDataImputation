{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0RQ0wjd696j",
        "outputId": "f4316c76-ff13-47e6-e375-3e1511ac78c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/imputation/* ."
      ],
      "metadata": {
        "id": "ZsmpVE9X68o9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AKL1BqLpOrry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import math\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "INPUT_SIZE = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "B5ezaSKiu0_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') \n",
        "    return device\n",
        "device = get_device()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPByV49cUi4-",
        "outputId": "27ec43ce-f2fb-4f65-f792-ebd44544b6f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(input_size)).to(device)\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        m = m.to(device)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(output_size)).to(device)\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size).to(device)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "class RITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE).to(device)\n",
        "\n",
        "        self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "        self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "        self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE).to(device)\n",
        "        self.feat_reg = FeatureRegression(INPUT_SIZE).to(device)\n",
        "\n",
        "        self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE).to(device)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.25).to(device)\n",
        "        self.out = nn.Linear(RNN_HID_SIZE, 1).to(device)\n",
        "\n",
        "    def forward(self, data, direct):\n",
        "        values = data[direct]['values'].to(device)\n",
        "        masks = data[direct]['masks'].to(device)\n",
        "        deltas = data[direct]['deltas'].to(device)\n",
        "\n",
        "        evals = data[direct]['evals'].to(device)\n",
        "        eval_masks = data[direct]['eval_masks'].to(device)\n",
        "\n",
        "        h = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "        c = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "          \n",
        "\n",
        "        x_loss = 0.0\n",
        "        y_loss = 0.0\n",
        "\n",
        "        imputations = []\n",
        "\n",
        "        for t in range(SEQ_LEN):\n",
        "            x = values[:, t, :]\n",
        "            m = masks[:, t, :]\n",
        "            d = deltas[:, t, :]\n",
        "\n",
        "            gamma_h = self.temp_decay_h(d)\n",
        "            gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "            h = h * gamma_h\n",
        "\n",
        "            x_h = self.hist_reg(h)\n",
        "            x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            x_c =  m * x +  (1 - m) * x_h\n",
        "\n",
        "            z_h = self.feat_reg(x_c)\n",
        "            x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))\n",
        "\n",
        "            c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "            x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "            inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "            h, c = self.rnn_cell(inputs, (h, c))\n",
        "\n",
        "            imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "        imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "        return {'loss': x_loss / SEQ_LEN,\n",
        "                'imputations': imputations,\n",
        "                'evals': evals, \n",
        "                'eval_masks': eval_masks}\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data, direct = 'forward')\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data):\n",
        "        ret_f = self.rits_f(data, 'forward')\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward'))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "\n",
        "class MySet2(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(MySet2, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        tens = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "        return_dict[dict_key] = tens \n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}\n",
        "\n",
        "def collate_fn2(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "      backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "skdVIxrTa9Qf"
      },
      "outputs": [],
      "source": [
        "indexes= {'calories':1,'carbs':2,'fat':3,'protein':4}\n",
        "def load_normalizations(file):\n",
        "  content = open(file,'rb')\n",
        "  ret = pickle.load(content)\n",
        "  content.close()\n",
        "  return ret\n",
        "normalizations = load_normalizations('brits_normalization.pickle')\n",
        "\n",
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]\n",
        "\n",
        "def get_missing_and_index(index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1) #get all rows that are missing to compare to ground truth\n",
        "  return index,filter\n",
        "\n",
        "def print_impu_real(ret,index_name):\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  px = pd.DataFrame()\n",
        "  px.insert(0,\"Real\",real.numpy())\n",
        "  px.insert(0,\"Imputation\",impu.detach().numpy())\n",
        "  return px\n",
        "\n",
        "def get_abs_error_val(ret, index_name):\n",
        "  \"\"\"Calculates the absolute error of the brits imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  return torch.abs(impu-real).mean().item()\n",
        "\n",
        "def get_abs_personal_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the personal mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean().item()\n",
        "\n",
        "def get_abs_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  forward = data['forward']\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  mean_erro = torch.abs(real-z).mean().item()\n",
        "  return mean_erro\n",
        "\n",
        "def get_rel_error_val(ret,index_name):\n",
        "  \"\"\"Calculates the relative error of the brits imputation for each missing value and returns the mean error\n",
        "  It uses the normalized values to remove problems with 0 division\n",
        "  \"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = ret['imputations'][:,:,index][filter]\n",
        "  real = ret['evals'][:,:,index][filter]\n",
        "  return torch.abs((impu-real)/(torch.abs(real)+1)).mean().item()\n",
        "\n",
        "def get_errors_impu(ret):\n",
        "  return_dict = {}\n",
        "  for index_name in indexes.keys():\n",
        "    #rel_err = get_rel_error_val(ret,index_name)\n",
        "    abs_err = get_abs_error_val(ret, index_name)\n",
        "    return_dict[index_name] = abs_err\n",
        "  return return_dict\n",
        "\n",
        "def get_errors_bench(data):\n",
        "  ret = {}\n",
        "  for index_name in indexes.keys():\n",
        "    abs_err = get_abs_mean_impu_error(data,index_name)\n",
        "    ret[index_name] = abs_err\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_norm = True\n",
        "path_train = './brits_train.pickle'\n",
        "path_test = './brits_test.pickle'\n",
        "if not use_norm:\n",
        "  path_train = './brits_train_nonnorm.pickle'\n",
        "  path_test = './brits_test_nonnorm.pickle'\n",
        "test_set = MySet2(path_test)\n",
        "train_set = MySet2(path_train)\n",
        "\n",
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "xW5WF4euqutu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    lr = 1e-3,\n",
        "    batch_size= 500,\n",
        "    num_workers=1,\n",
        "    dataset=\"My Fitnesspal Small\",\n",
        "    epochs=100,\n",
        "    test_set_size= len(test_set),\n",
        "    train_set_size= len(train_set),\n",
        "    early_stop_after=2\n",
        ")"
      ],
      "metadata": {
        "id": "1BroW_JYqPZ5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "run = wandb.init(project=\"brits\", entity=\"gege-hoho\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "mUi5hi87meuW",
        "outputId": "666a8b25-e8d9-4d94-98ff-b2cef795036d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgege-hoho\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/gege-hoho/brits/runs/28vjb1hh\" target=\"_blank\">floral-gorge-1</a></strong> to <a href=\"https://wandb.ai/gege-hoho/brits\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = config['lr'] )\n",
        "\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = config['batch_size'],\n",
        "                        shuffle = True,pin_memory = True, \n",
        "                        collate_fn = collate_fn2, \n",
        "                        num_workers=config[\"num_workers\"])\n",
        "\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = config['batch_size'],shuffle = False,pin_memory = True, collate_fn = collate_fn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RxB3iYgjRfGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c69e34-f95c-46e4-8ec5-50c833d7ded9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0 29.397570848464966\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "last_errors = {}\n",
        "for epoch in range(config['epochs']):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "    train_loss += ret['loss'].item()\n",
        "  train_loss = train_loss/(idx + 1.0)\n",
        "  wandb.log({\"train-loss\": train_loss})\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  for idx,data in enumerate(test_iter):\n",
        "    ret = model.run_on_batch(data, None)\n",
        "    test_loss += ret['loss'].item()\n",
        "  test_loss = test_loss/(idx+ 1.0)\n",
        "  test_errors = get_errors_impu(ret)\n",
        "  wandb.log({\"test-loss\": test_loss})\n",
        "  wandb.log({\"test-errors\": test_errors})\n",
        "\n",
        "  #early stopping\n",
        "  \n",
        "\n",
        "print(f\"{device} {time.time()-t0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_errors_bench(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXHguq-ax9FW",
        "outputId": "40d51a9a-ff88-49a1-d3d6-d3236c27b7e2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'calories': {'abs': tensor(249.2644)},\n",
              " 'carbs': {'abs': tensor(29.9034)},\n",
              " 'fat': {'abs': tensor(12.1436)},\n",
              " 'protein': {'abs': tensor(18.6539)}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_errors_impu(ret)['calories']['abs']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGDNiRFQLEnt",
        "outputId": "acaf4ead-235a-4918-be13-091aa9b2c0db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192.71902465820312"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index_name = 'calories'\n",
        "mean_erro = get_abs_mean_impu_error(data,index_name)\n",
        "personal_mean_erro = get_abs_personal_mean_impu_error(data,index_name)\n",
        "rnn_erro = get_abs_error_val(ret,index_name)\n",
        "\n",
        "print(f\"RNN   {rnn_erro}\")\n",
        "print(f\"MEAN  {mean_erro}\")\n",
        "print(f\"MEANP {personal_mean_erro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4-NcN_tjf4",
        "outputId": "0e2c77d2-86f7-4b31-a9ac-08e4d4e7db00"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN   192.71902465820312\n",
            "MEAN  249.26441955566406\n",
            "MEANP 259.15252685546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use_norm = True\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "  \n",
        "\n",
        "in_file =  open(path_test,'rb')\n",
        "recs = pickle.load(in_file)\n",
        "in_file.close()\n",
        "#recs = [json.loads(x) for x in content]\n",
        "recs = [x['forward'] for x in recs]\n",
        "#recs = [[(y['values'][indexes['calories']] - normalizations['mean']['calories'] )/ normalizations['std']['calories'] for y in x]for x in recs]\n",
        "#recs = flatten(recs)\n",
        "#recs = [revert_norm(x,'calories') for x in recs]\n",
        "recs = [[revert_norm(y['values'][indexes['calories']],'calories')for y in x]for x in recs]\n",
        "recs = flatten(recs)\n",
        "recs = np.array(recs)\n",
        "recs.mean()"
      ],
      "metadata": {
        "id": "vk50FGc292jD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}