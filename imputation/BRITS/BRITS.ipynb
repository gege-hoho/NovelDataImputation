{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKL1BqLpOrry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import ujson as json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "INPUT_SIZE = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "B5ezaSKiu0_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size))\n",
        "        self.b = Parameter(torch.Tensor(input_size))\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size))\n",
        "        self.b = Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "class RITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE)\n",
        "\n",
        "        self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "        self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "        self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE)\n",
        "        self.feat_reg = FeatureRegression(INPUT_SIZE)\n",
        "\n",
        "        self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.25)\n",
        "        self.out = nn.Linear(RNN_HID_SIZE, 1)\n",
        "\n",
        "    def forward(self, data, direct):\n",
        "        values = data[direct]['values']\n",
        "        masks = data[direct]['masks']\n",
        "        deltas = data[direct]['deltas']\n",
        "\n",
        "        evals = data[direct]['evals']\n",
        "        eval_masks = data[direct]['eval_masks']\n",
        "\n",
        "        h = torch.zeros((values.size()[0], RNN_HID_SIZE))\n",
        "        c = torch.zeros((values.size()[0], RNN_HID_SIZE))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "\n",
        "        x_loss = 0.0\n",
        "        y_loss = 0.0\n",
        "\n",
        "        imputations = []\n",
        "\n",
        "        for t in range(SEQ_LEN):\n",
        "            x = values[:, t, :]\n",
        "            m = masks[:, t, :]\n",
        "            d = deltas[:, t, :]\n",
        "\n",
        "            gamma_h = self.temp_decay_h(d)\n",
        "            gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "            h = h * gamma_h\n",
        "\n",
        "            x_h = self.hist_reg(h)\n",
        "            x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            x_c =  m * x +  (1 - m) * x_h\n",
        "\n",
        "            z_h = self.feat_reg(x_c)\n",
        "            x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))\n",
        "\n",
        "            c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "            x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "            inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "            h, c = self.rnn_cell(inputs, (h, c))\n",
        "\n",
        "            imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "        imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "        return {'loss': x_loss / SEQ_LEN,\n",
        "                'imputations': imputations,\n",
        "                'evals': evals, \n",
        "                'eval_masks': eval_masks}\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data, direct = 'forward')\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data):\n",
        "        ret_f = self.rits_f(data, 'forward')\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward'))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_normalizations(file):\n",
        "  content = open(file).readlines()\n",
        "  return json.loads(content[0])"
      ],
      "metadata": {
        "id": "rxFsveZ7EBzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "class MySet2(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(MySet2, self).__init__()\n",
        "        content = open(content_path).readlines()\n",
        "        recs = [json.loads(x) for x in content]\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        return_dict[dict_key] = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn2(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "    forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "    backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "abyIB8rswJMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-nIWo1KUc2R"
      },
      "outputs": [],
      "source": [
        "indexes= {'meal':0,'calories':1,'carbs':2,'fat':3,'protein':4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "batch_size = 64\n",
        "use_norm = True\n",
        "path_train_json = './brits_json.json'\n",
        "path_test_json = './brits_test.json'\n",
        "if not use_norm:\n",
        "  path_train_json = './brits_json_nonnorm.json'\n",
        "  path_test_json = './brits_test_nonnorm.json'\n",
        "normalizations = load_normalizations('brits_normalization.json')\n",
        "\n",
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "train_set = MySet2(path_train_json)\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = batch_size,shuffle = True,pin_memory = True, collate_fn = collate_fn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxB3iYgjRfGD"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  run_loss = 0.0\n",
        "\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "\n",
        "    run_loss += ret['loss'].item()\n",
        "\n",
        "    print('\\r Progress epoch {}, {:.2f}%, average loss {}'.format(epoch, (idx + 1) * 100.0 / len(train_iter), run_loss / (idx + 1.0))),"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5UUnb13QYib"
      },
      "outputs": [],
      "source": [
        "test_set = MySet2(path_test_json)\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = batch_size,shuffle = False,pin_memory = True, collate_fn = collate_fn2)\n",
        "model.eval()\n",
        "for idx, data in enumerate(test_iter):\n",
        "  ret = model.run_on_batch(data, None)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "ogmrBN84SqF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]"
      ],
      "metadata": {
        "id": "uf2tsKatEPel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skdVIxrTa9Qf",
        "outputId": "f4346c75-d2e3-461e-9f0a-b531e87e9829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN   341.09503173828125\n",
            "MEAN  295.6275939941406\n",
            "MEANP 304.252685546875\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def calc_error_on_row(ret, index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1)#get all rows that are missing to compare to ground truth\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  return torch.abs(impu-real).mean()\n",
        "\n",
        "def calculate_personal_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean()\n",
        "\n",
        "def calc_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  filter = (forward['eval_masks'][:,:,index]==1)#get all rows that are missing\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  #mean_erro = torch.abs(real-z).mean()\n",
        "  mean_erro = torch.abs(real-z).mean()\n",
        "  return mean_erro\n",
        "\n",
        "index_name = 'calories'\n",
        "mean_erro = calc_mean_error(data,index_name)\n",
        "personal_mean_erro = calculate_personal_mean_error(data,index_name)\n",
        "rnn_erro = calc_error_on_row(ret,index_name)\n",
        "\n",
        "print(f\"RNN   {rnn_erro}\")\n",
        "print(f\"MEAN  {mean_erro}\")\n",
        "print(f\"MEANP {personal_mean_erro}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizations"
      ],
      "metadata": {
        "id": "X6Vi8cojJJKs",
        "outputId": "ae399221-af18-419b-969e-f7993771cf23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean': {'calories': 569.9514961389962,\n",
              "  'carbs': 57.65986969111969,\n",
              "  'cholest': 82.33252895752896,\n",
              "  'fat': 23.61534749034749,\n",
              "  'fiber': 6.033301158301159,\n",
              "  'protein': 29.91686776061776,\n",
              "  'sodium': 737.3712596525097,\n",
              "  'sugar': 20.981901544401545},\n",
              " 'std': {'calories': 19.033275991779742,\n",
              "  'carbs': 6.547162660719812,\n",
              "  'cholest': 11.447796506294267,\n",
              "  'fat': 4.473921516546562,\n",
              "  'fiber': 2.448608392557053,\n",
              "  'protein': 4.818375431259857,\n",
              "  'sodium': 31.317557844872915,\n",
              "  'sugar': 4.7081826053650495}}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}