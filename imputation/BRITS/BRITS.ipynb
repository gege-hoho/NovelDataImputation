{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b0RQ0wjd696j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d23870d-2f97-4425-8be0-48d700d3d395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/imputation/* ."
      ],
      "metadata": {
        "id": "ZsmpVE9X68o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab\n",
        "Import used packages"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKL1BqLpOrry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21dc785e-c391-4e81-dda2-35058a9d85f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define variables for the Imputation network\n",
        "\n",
        "\n",
        "*   SEQ_LEN: number of entries in one sequence 7 days * 4 meals = 28\n",
        "*   RNN_HID_SIZE: size of the hidden state, 64 standard from BRITS\n",
        "*   no_classes: number of food categories set to 0 if not used\n",
        "*   END_REG: Index at which the regression variables end(exclusive) and the categorical variables start\n",
        "*   use_categorical_loss: If BCE loss should be used for categories\n",
        "*   use_norm: If normalised data is used (does not work well with unnormalised)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z41se5Ln98vZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "no_classes = 0\n",
        "END_REG = 10 #\n",
        "INPUT_SIZE = END_REG + no_classes\n",
        "use_categorical_loss = False\n",
        "use_norm = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') \n",
        "    return device\n",
        "device = get_device()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPByV49cUi4-",
        "outputId": "f6eb0943-fc74-473c-cca4-0595877b1ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network Classes\n",
        "\n",
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3 and Pytorch 1.10:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "sqTP34dWCt5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Regression and Decay"
      ],
      "metadata": {
        "id": "FwCHr2UChRNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(input_size)).to(device)\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        m = m.to(device)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(output_size)).to(device)\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size).to(device)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RITS and BRITS"
      ],
      "metadata": {
        "id": "XOzYvp1ChXx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "if not use_categorical_loss:\n",
        "  class RITS(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(RITS, self).__init__()\n",
        "          self.build()\n",
        "\n",
        "      def build(self):\n",
        "          self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE).to(device)\n",
        "\n",
        "          self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "          self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "          self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE).to(device)\n",
        "          self.feat_reg = FeatureRegression(INPUT_SIZE).to(device)\n",
        "\n",
        "          self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE).to(device)\n",
        "\n",
        "          self.dropout = nn.Dropout(p = 0.25).to(device)\n",
        "          self.out = nn.Linear(RNN_HID_SIZE, 1).to(device)\n",
        "\n",
        "      def forward(self, data, direct, is_test = False):\n",
        "          values = data[direct]['values'].to(device)\n",
        "          masks = data[direct]['masks'].to(device)\n",
        "          deltas = data[direct]['deltas'].to(device)\n",
        "          \n",
        "          evals = None\n",
        "          eval_masks = None\n",
        "          if not is_test:\n",
        "            evals = data[direct]['evals'].to(device)\n",
        "            eval_masks = data[direct]['eval_masks'].to(device)\n",
        "\n",
        "          h = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "          c = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "            \n",
        "\n",
        "          x_loss = torch.FloatTensor([0.0]).to(device)\n",
        "          cat_loss = torch.FloatTensor([0.0]).to(device)\n",
        "\n",
        "          imputations = []\n",
        "\n",
        "          for t in range(SEQ_LEN):\n",
        "              x = values[:, t, :]\n",
        "              m = masks[:, t, :]\n",
        "              d = deltas[:, t, :]\n",
        "\n",
        "              gamma_h = self.temp_decay_h(d)\n",
        "              gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "              h = h * gamma_h\n",
        "\n",
        "              x_h = self.hist_reg(h)#(1)\n",
        "              x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)#(5)\n",
        "\n",
        "              x_c =  m * x +  (1 - m) * x_h #(2)\n",
        "\n",
        "              z_h = self.feat_reg(x_c) #(7) Estimation regarding current timestep features \n",
        "              x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "              alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1)) #(8)called beta in paper: Wheight that combines history based estimation x_h and feature based estimation z_h\n",
        "\n",
        "              c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "              x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "              c_c = m * x + (1 - m) * c_h # 10 \n",
        "\n",
        "              inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "              h, c = self.rnn_cell(inputs, (h, c)) #RNN step\n",
        "\n",
        "              imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "          imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "          return {'loss': x_loss / SEQ_LEN,\n",
        "                  'x_loss': x_loss/ SEQ_LEN,\n",
        "                  'cat_loss': cat_loss / SEQ_LEN,\n",
        "                  'imputations': imputations,\n",
        "                  'evals': evals, \n",
        "                  'eval_masks': eval_masks}\n",
        "\n",
        "      def run_on_batch(self, data, optimizer):\n",
        "          ret = self(data, direct = 'forward')\n",
        "\n",
        "          if optimizer is not None:\n",
        "              optimizer.zero_grad()\n",
        "              ret['loss'].backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          return ret\n",
        "else:\n",
        "  class RITS(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(RITS, self).__init__()\n",
        "          self.build()\n",
        "\n",
        "      def build(self):\n",
        "          self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE).to(device)\n",
        "\n",
        "          self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "          self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "          self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE).to(device)\n",
        "          self.feat_reg = FeatureRegression(INPUT_SIZE).to(device)\n",
        "\n",
        "          self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE).to(device)\n",
        "\n",
        "          self.dropout = nn.Dropout(p = 0.25).to(device)\n",
        "          self.out = nn.Linear(RNN_HID_SIZE, 1).to(device)\n",
        "\n",
        "      def forward(self, data, direct, is_test = False):\n",
        "          values = data[direct]['values'].to(device)\n",
        "          masks = data[direct]['masks'].to(device)\n",
        "          deltas = data[direct]['deltas'].to(device)\n",
        "          \n",
        "          evals = None\n",
        "          eval_masks = None\n",
        "          if not is_test:\n",
        "            evals = data[direct]['evals'].to(device)\n",
        "            eval_masks = data[direct]['eval_masks'].to(device)\n",
        "\n",
        "          h = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "          c = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "            \n",
        "\n",
        "          x_loss = torch.FloatTensor([0.0]).to(device)\n",
        "          cat_loss = torch.FloatTensor([0.0]).to(device)\n",
        "\n",
        "          imputations = []\n",
        "\n",
        "          for t in range(SEQ_LEN):\n",
        "              x = values[:, t,:]\n",
        "              m = masks[:, t,:]\n",
        "              d = deltas[:, t,:]\n",
        "\n",
        "              gamma_h = self.temp_decay_h(d)\n",
        "              gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "              h = h * gamma_h\n",
        "\n",
        "              x_h = self.hist_reg(h)\n",
        "\n",
        "              x_loss += torch.sum(torch.abs(x[:,:END_REG] - x_h[:,:END_REG]) * m[:,:END_REG]) / (torch.sum(m[:,:END_REG]) + 1e-5)\n",
        "              x_c =  m * x +  (1 - m) * x_h # (1) take all actual values from x and all missing from approximation x_h (xhat)\n",
        "\n",
        "              z_h = self.feat_reg(x_c) # (7) in paper\n",
        "              x_loss += torch.sum(torch.abs(x[:,:END_REG] - z_h[:,:END_REG]) * m[:,:END_REG]) / (torch.sum(m[:,:END_REG]) + 1e-5)\n",
        "\n",
        "              alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))# called beta in the paper wheigh\n",
        "\n",
        "              c_h = alpha * z_h + (1 - alpha) * x_h #(9)\n",
        "              x_loss += torch.sum(torch.abs(x[:,:END_REG] - c_h[:,:END_REG]) * m[:,:END_REG]) / (torch.sum(m[:,:END_REG]) + 1e-5)\n",
        "\n",
        "              #categorical loss only if there are categorical values\n",
        "              if END_REG < INPUT_SIZE:\n",
        "                filter = [m[:,END_REG:]==1]\n",
        "                x_filter = x[:,END_REG:][filter]\n",
        "                c_h_filter = c_h[:,END_REG:][filter]\n",
        "                cat_loss += F.binary_cross_entropy_with_logits(c_h_filter,x_filter)\n",
        "\n",
        "              c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "              inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "              h, c = self.rnn_cell(inputs, (h, c))\n",
        "              imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "          imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "          return {'loss': (x_loss+cat_loss) / SEQ_LEN,\n",
        "                  'x_loss': x_loss/ SEQ_LEN,\n",
        "                  'cat_loss': cat_loss / SEQ_LEN,\n",
        "                  'imputations': imputations,\n",
        "                  'evals': evals, \n",
        "                  'eval_masks': eval_masks}\n",
        "\n",
        "      def run_on_batch(self, data, optimizer):\n",
        "          ret = self(data, direct = 'forward')\n",
        "\n",
        "          if optimizer is not None:\n",
        "              optimizer.zero_grad()\n",
        "              ret['loss'].backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data, is_test = False):\n",
        "        ret_f = self.rits_f(data, 'forward',is_test)\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward',is_test))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_ is None:\n",
        "              return tensor_\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "\n",
        "class BRITSCategoriesSet(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(BRITSCategoriesSet, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        tens = torch.FloatTensor([[x[dict_key][:END_REG]for x in r]for r in recs])\n",
        "        bam = torch.zeros(len(recs),SEQ_LEN,no_classes)\n",
        "        for idx,r in enumerate(recs):\n",
        "          for idx2,x in enumerate(r):\n",
        "            if dict_key in ('masks','deltas','eval_masks'):\n",
        "              bam[idx][idx2] = x[dict_key][END_REG]\n",
        "            else:  \n",
        "              bam[idx][idx2] = 0\n",
        "              for cat in x[dict_key][END_REG:]:\n",
        "                cat = int(cat)\n",
        "                if cat == -1:\n",
        "                  continue\n",
        "                bam[idx][idx2] += F.one_hot(torch.Tensor([cat]).to(torch.int64),no_classes).squeeze()\n",
        "        return_dict[dict_key] = torch.cat((tens,bam),dim=2) \n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}\n",
        "\n",
        "\n",
        "class BRITSSet(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(BRITSSet, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        tens = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "        return_dict[dict_key] = tens \n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}\n",
        "\n",
        "def collate_fn(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "      backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Functions"
      ],
      "metadata": {
        "id": "gTlYptSrDaqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skdVIxrTa9Qf"
      },
      "outputs": [],
      "source": [
        "indexes= {'calories':1,'carbs':2,'fat':3,'protein':4}\n",
        "def load_normalizations(file):\n",
        "  content = open(file,'rb')\n",
        "  ret = pickle.load(content)\n",
        "  content.close()\n",
        "  return ret\n",
        "normalizations = load_normalizations('brits_normalization.pickle')\n",
        "\n",
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]\n",
        "\n",
        "def get_missing_and_index(data, index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (data['forward']['eval_masks'][:,:,index]==1) #get all rows that are missing to compare to ground truth\n",
        "  return index,filter\n",
        "\n",
        "def print_impu_real(ret,index_name):\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  px = pd.DataFrame()\n",
        "  px.insert(0,\"Real\",real.cpu().numpy())\n",
        "  px.insert(0,\"Imputation\",impu.cpu().detach().numpy())\n",
        "  return px\n",
        "\n",
        "def get_abs_error_val(ret, data, index_name):\n",
        "  \"\"\"Calculates the absolute error of the brits imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name).cpu()\n",
        "  real = revert_norm(data['forward']['evals'][:,:,index][filter],index_name).cpu()\n",
        "  abs = torch.abs(impu-real)\n",
        "  return abs.mean().item(),abs.std().item()\n",
        "\n",
        "def get_abs_personal_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the personal mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean().item(),error.std().item()\n",
        "\n",
        "def get_impu_and_real(data,ret,index_name):\n",
        "  \"\"\"Returns the imputated and the real value for all missing for a specific index\"\"\"\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name).cpu().detach().numpy()\n",
        "  real = revert_norm(data['forward']['evals'][:,:,index][filter],index_name).cpu().detach().numpy()\n",
        "  return impu,real\n",
        "\n",
        "def get_abs_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  forward = data['forward']\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  abs = torch.abs(real-z)\n",
        "  return abs.mean().item(),abs.std().item()\n",
        "\n",
        "def get_abs__meal_mean_impu_error(data,index_name):\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  forward = data['forward']\n",
        "  meal_means = {}\n",
        "  for x in range(1,5):\n",
        "    meal_index = 0\n",
        "    #mean of specific meal \n",
        "    meal_mean = revert_norm(forward['values'][:,:,index][(forward['evals'][:,:,meal_index]==x)&(filter==False)],index_name).mean().item()\n",
        "    meal_means[x] = meal_mean\n",
        "  meals = forward['evals'][:,:,meal_index][filter] #meals of all missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  aprox = torch.empty(len(meals))\n",
        "  for x in range(len(meals)):\n",
        "    aprox[x] = meal_means[int(meals[x].item())]\n",
        "  abs = torch.abs(real-aprox)\n",
        "  return abs.mean().item(),abs.std().item()\n",
        "\n",
        "\n",
        "def get_rel_error_val(ret, data, index_name):\n",
        "  \"\"\"Calculates the relative error of the brits imputation for each missing value and returns the mean error\n",
        "  It uses the normalized values to remove problems with 0 division\n",
        "  \"\"\"\n",
        "  index, filter = get_missing_and_index(data, index_name)\n",
        "  impu = ret['imputations'][:,:,index][filter]\n",
        "  real = data['evals'][:,:,index][filter]\n",
        "  return torch.abs((impu-real)/(torch.abs(real)+1)).mean().item()\n",
        "\n",
        "def get_errors_impu(ret,data):\n",
        "  return_dict = {}\n",
        "  for index_name in indexes.keys():\n",
        "    #rel_err = get_rel_error_val(ret,index_name)\n",
        "    abs_err,_ = get_abs_error_val(ret, data, index_name)\n",
        "    return_dict[index_name] = abs_err\n",
        "  return return_dict\n",
        "\n",
        "def get_errors_bench(data):\n",
        "  ret = {}\n",
        "  for index_name in indexes.keys():\n",
        "    abs_err = get_abs_mean_impu_error(data,index_name)\n",
        "    ret[index_name] = abs_err\n",
        "  return ret\n",
        "def clean_data(data):\n",
        "  \"\"\"Removes eval and eval_masks from data before sending it to the model\"\"\"\n",
        "  forward_clean = data['forward'].copy()\n",
        "  backward_clean = data['backward'].copy()\n",
        "  del forward_clean['evals']\n",
        "  del forward_clean['eval_masks']\n",
        "  del backward_clean['evals']\n",
        "  del backward_clean['eval_masks']\n",
        "  return {'forward':forward_clean,'backward': backward_clean}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "R8GWstVcDeEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = './brits_train.pickle'\n",
        "path_test = './brits_test.pickle'\n",
        "if not use_norm:\n",
        "  path_train = './brits_train_nonnorm.pickle'\n",
        "  path_test = './brits_test_nonnorm.pickle'\n",
        "\n",
        "if not use_categorical_loss:\n",
        "  test_set = BRITSSet(path_test)\n",
        "  train_set = BRITSSet(path_train)\n",
        "else:\n",
        "  test_set = BRITSCategoriesSet(path_test)\n",
        "  train_set = BRITSCategoriesSet(path_train)\n",
        "\n",
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "xW5WF4euqutu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    lr = 1e-3,\n",
        "    batch_size= 10000,\n",
        "    use_norm = use_norm,\n",
        "    num_workers=2,\n",
        "    dataset=\"My Fitnesspal small\",\n",
        "    epochs=1200,\n",
        "    test_set_size= len(test_set),\n",
        "    train_set_size= len(train_set)\n",
        ")"
      ],
      "metadata": {
        "id": "1BroW_JYqPZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_wandb = True\n",
        "if use_wandb:\n",
        "  wandb.login()\n",
        "  run = wandb.init(project=\"brits-big-u2\", entity=\"gege-hoho\",name=\"small totally random\", config=config)\n",
        "print(f\"Test Set Size: {len(test_set)}\")\n",
        "print(f\"Train Set Size: {len(train_set)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "mUi5hi87meuW",
        "outputId": "baa5465d-28a9-481c-f1d2-2c1b2d6e5577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220317_131800-2t9s1deh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/gege-hoho/brits-big-u2/runs/2t9s1deh\" target=\"_blank\">small totally random</a></strong> to <a href=\"https://wandb.ai/gege-hoho/brits-big-u2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Size: 60\n",
            "Train Set Size: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = config['lr'] )\n",
        "\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = config['batch_size'],\n",
        "                        shuffle = True,pin_memory = True, \n",
        "                        collate_fn = collate_fn, \n",
        "                        num_workers=config[\"num_workers\"])\n",
        "\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = config['batch_size'],shuffle = False,pin_memory = True, collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_errors = None\n",
        "early_stopping_dev = 0.05"
      ],
      "metadata": {
        "id": "vvp7kaPipOni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxB3iYgjRfGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "9d41ed66-47e2-42f0-bb74-2558dfd41b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▊    | 703/1200 [04:53<03:27,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ba9950685ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bdc3a049f122>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, is_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mret_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrits_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mret_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrits_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'backward'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_ret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-237c65df28eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, direct, is_test)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0mz_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(7) Estimation regarding current timestep features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m               \u001b[0mx_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m               \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(8)called beta in paper: Wheight that combines history based estimation x_h and feature based estimation z_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "early_stopping = False\n",
        "last_errors = {}\n",
        "for epoch in tqdm(range(config['epochs'])):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "    train_loss += ret['loss'].item()\n",
        "  train_loss = train_loss/(idx + 1.0)\n",
        "  if use_wandb:\n",
        "    wandb.log({\"train-loss\": train_loss})\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  for idx,data in enumerate(test_iter):\n",
        "    data_clean = clean_data(data)\n",
        "    ret = model(data_clean,is_test=True)\n",
        "    test_loss += ret['loss'].item()\n",
        "  test_loss = test_loss/(idx+ 1.0)\n",
        "  test_errors = get_errors_impu(ret,data)\n",
        "  if use_wandb:\n",
        "    wandb.log({\"test-loss\": test_loss})\n",
        "    wandb.log({\"test-errors\": test_errors})\n",
        "    #wandb.log({\"cat_loss\": ret['cat_loss']})\n",
        "    #wandb.log({\"x_loss\": ret['x_loss']})\n",
        "  else:\n",
        "    print(test_errors)\n",
        "  \n",
        "  #early stopping\n",
        "  if best_errors is None:\n",
        "    best_errors = test_errors.copy()\n",
        "  for k,v in test_errors.items():\n",
        "    if best_errors[k] > v:\n",
        "      best_errors[k] = v\n",
        "    if best_errors[k] * (1.0 + early_stopping_dev) < v:\n",
        "      print(f\"Key {k} deviates to strong from best measured value: {((v*1.0)/best_errors[k]):.2f}\")\n",
        "      print(f\"Best measured: {best_errors[k]:.2f}\")\n",
        "      print(f\"Current: {v:.2f}\")\n",
        "      early_stopping = True\n",
        "  if early_stopping:\n",
        "    early_stopping = False\n",
        "    break\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"{device} {time.time()-t0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_iter = DataLoader(dataset = test_set,batch_size = config['batch_size'],shuffle = False,pin_memory = True, collate_fn = collate_fn)\n",
        "model.eval()\n",
        "for idx, data in enumerate(test_iter):\n",
        "    data_clean = clean_data(data)\n",
        "    ret = model(data_clean,is_test=True)\n",
        "    break\n",
        "for index_name,_ in indexes.items():\n",
        "  mean_erro = get_abs_mean_impu_error(data,index_name)\n",
        "  personal_mean_erro = get_abs_personal_mean_impu_error(data,index_name)\n",
        "  meal_impu_erro = get_abs__meal_mean_impu_error(data,index_name)\n",
        "  rnn_erro = get_abs_error_val(ret,data,index_name)\n",
        "\n",
        "\n",
        "  print(f\"{index_name}:\")\n",
        "  print(f\"RNN    {rnn_erro}\")\n",
        "  print(f\"MEAN   {mean_erro}\")\n",
        "  print(f\"MEANP  {personal_mean_erro}\")\n",
        "  print(f\"MEANM  {meal_impu_erro}\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4-NcN_tjf4",
        "outputId": "6de9d406-85e8-4b07-cbb5-9113180cbb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calories:\n",
            "RNN    (217.2267608642578, 223.89520263671875)\n",
            "MEAN   (243.46044921875, 252.0259246826172)\n",
            "MEANP  (245.46922302246094, 233.00062561035156)\n",
            "MEANM  (230.98062133789062, 238.9537811279297)\n",
            "\n",
            "carbs:\n",
            "RNN    (26.618669509887695, 27.924129486083984)\n",
            "MEAN   (30.717683792114258, 29.576753616333008)\n",
            "MEANP  (29.398717880249023, 28.87196922302246)\n",
            "MEANM  (29.76840591430664, 29.639984130859375)\n",
            "\n",
            "fat:\n",
            "RNN    (12.06336498260498, 11.073168754577637)\n",
            "MEAN   (14.199101448059082, 12.544651985168457)\n",
            "MEANP  (12.51538372039795, 12.273148536682129)\n",
            "MEANM  (13.640007972717285, 12.061634063720703)\n",
            "\n",
            "protein:\n",
            "RNN    (11.524516105651855, 12.383269309997559)\n",
            "MEAN   (14.460770606994629, 14.228263854980469)\n",
            "MEANP  (13.229487419128418, 13.50601577758789)\n",
            "MEANM  (13.39605712890625, 12.805740356445312)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model to google drive and print the model name to log\n",
        "\n"
      ],
      "metadata": {
        "id": "sxkLL2RlW1L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_drive_path = '/content/drive/MyDrive/Uni/Masterarbeit/data/britsmodels'\n",
        "name_save = f\"{g_drive_path}/brits{config['dataset']}{datetime.now()}\"\n",
        "torch.save(model,name_save)\n",
        "print(name_save)\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "8b856879f7ca42629c910bb0b6c94249",
            "0b03eb789aaf417ea4127a3af4663c9c",
            "da21ad3f4ed046b9a33125255743482d",
            "3e71c917cb4341c1bab6ef562beceb1a",
            "ca509e23cf894f3595a407d79e0ceea4",
            "6a600372b1ce479aa8b1c33e3fb16e03",
            "a5d22f231b8b40cab8b15c48f3c9c089",
            "b788f56007c849f89dd17f80763ec541"
          ]
        },
        "id": "TDAgZltk9f9Y",
        "outputId": "1a39d2e1-af8d-4bf2-bd74-c5b5c4a37d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Uni/Masterarbeit/data/britsmodels/britsMy Fitnesspal small2022-03-17 13:23:07.623939\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.360 MB of 0.360 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b856879f7ca42629c910bb0b6c94249"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test-loss</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train-loss</td><td>█▇▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test-loss</td><td>32.98052</td></tr><tr><td>train-loss</td><td>31.60585</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">small totally random</strong>: <a href=\"https://wandb.ai/gege-hoho/brits-big-u2/runs/2t9s1deh\" target=\"_blank\">https://wandb.ai/gege-hoho/brits-big-u2/runs/2t9s1deh</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220317_131800-2t9s1deh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model from drive for further testing"
      ],
      "metadata": {
        "id": "OQfOFpeABLgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_drive_path = '/content/drive/MyDrive/Uni/Masterarbeit/data/britsmodels'\n",
        "name_load = name_save #f\"{g_drive_path}/britsMy Fitnesspal BIG u22022-03-17 09:56:35.266689\"\n",
        "model = torch.load(name_load)"
      ],
      "metadata": {
        "id": "qQOOjAKoj_US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import rcParams\n",
        "\n",
        "for data in test_iter:\n",
        "  break\n",
        "model.eval()\n",
        "font_size = 6.5\n",
        "rcParams.update({'font.size':font_size})\n",
        "ret = model(data,is_test=True)\n",
        "impu, real = get_impu_and_real(data,ret,'calories')\n",
        "x  = pd.DataFrame({'impu': impu,'real':real,'error': impu-real,'abs_error':np.abs(impu-real)})\n",
        "x.sort_values(by=['error'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.8, 3))\n",
        "ax.hist(x['error'],bins=200)\n",
        "ax.set_xlabel(\"calories error\")\n",
        "ax.set_ylabel(\"# of occurences\")\n",
        "fig.tight_layout()\n",
        "plt.savefig(\"bins.pdf\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "TnGxXNTBvohZ",
        "outputId": "f7fd8db2-5484-4660-a73e-f1e646acf807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 417.6x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}