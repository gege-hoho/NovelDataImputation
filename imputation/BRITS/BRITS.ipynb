{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b0RQ0wjd696j",
        "outputId": "e4683b06-cd49-4e5b-8f9b-ce6eeb2c92ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/imputation/* ."
      ],
      "metadata": {
        "id": "ZsmpVE9X68o9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AKL1BqLpOrry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "INPUT_SIZE = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "B5ezaSKiu0_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size))\n",
        "        self.b = Parameter(torch.Tensor(input_size))\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size))\n",
        "        self.b = Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "class RITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE)\n",
        "\n",
        "        self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "        self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "        self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE)\n",
        "        self.feat_reg = FeatureRegression(INPUT_SIZE)\n",
        "\n",
        "        self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.25)\n",
        "        self.out = nn.Linear(RNN_HID_SIZE, 1)\n",
        "\n",
        "    def forward(self, data, direct):\n",
        "        values = data[direct]['values']\n",
        "        masks = data[direct]['masks']\n",
        "        deltas = data[direct]['deltas']\n",
        "\n",
        "        evals = data[direct]['evals']\n",
        "        eval_masks = data[direct]['eval_masks']\n",
        "\n",
        "        h = torch.zeros((values.size()[0], RNN_HID_SIZE))\n",
        "        c = torch.zeros((values.size()[0], RNN_HID_SIZE))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "\n",
        "        x_loss = 0.0\n",
        "        y_loss = 0.0\n",
        "\n",
        "        imputations = []\n",
        "\n",
        "        for t in range(SEQ_LEN):\n",
        "            x = values[:, t, :]\n",
        "            m = masks[:, t, :]\n",
        "            d = deltas[:, t, :]\n",
        "\n",
        "            gamma_h = self.temp_decay_h(d)\n",
        "            gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "            h = h * gamma_h\n",
        "\n",
        "            x_h = self.hist_reg(h)\n",
        "            x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            x_c =  m * x +  (1 - m) * x_h\n",
        "\n",
        "            z_h = self.feat_reg(x_c)\n",
        "            x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))\n",
        "\n",
        "            c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "            x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "            inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "            h, c = self.rnn_cell(inputs, (h, c))\n",
        "\n",
        "            imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "        imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "        return {'loss': x_loss / SEQ_LEN,\n",
        "                'imputations': imputations,\n",
        "                'evals': evals, \n",
        "                'eval_masks': eval_masks}\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data, direct = 'forward')\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data):\n",
        "        ret_f = self.rits_f(data, 'forward')\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward'))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_normalizations(file):\n",
        "  content = open(file,'rb')\n",
        "  ret = pickle.load(content)\n",
        "  content.close()\n",
        "  return ret"
      ],
      "metadata": {
        "id": "rxFsveZ7EBzP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "class MySet2(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(MySet2, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        return_dict[dict_key] = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn2(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "    forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "    backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "abyIB8rswJMB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A-nIWo1KUc2R"
      },
      "outputs": [],
      "source": [
        "indexes= {'meal':0,'calories':1,'carbs':2,'fat':3,'protein':4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "batch_size = 64\n",
        "use_norm = True\n",
        "path_train = './brits_train.pickle'\n",
        "path_test = './brits_test.pickle'\n",
        "if not use_norm:\n",
        "  path_train = './brits_train_nonnorm.pickle'\n",
        "  path_test = './brits_test_nonnorm.pickle'\n",
        "normalizations = load_normalizations('brits_normalization.pickle')\n",
        "\n",
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "train_set = MySet2(path_train)\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = batch_size,shuffle = True,pin_memory = True, collate_fn = collate_fn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RxB3iYgjRfGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d953c5-4462-4a80-d0ef-9c45894d8bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r Progress epoch 0, 20.00%, average loss 48.127769470214844\n",
            " Progress epoch 0, 40.00%, average loss 48.00994110107422\n",
            " Progress epoch 0, 60.00%, average loss 47.89475758870443\n",
            " Progress epoch 0, 80.00%, average loss 47.781012535095215\n",
            " Progress epoch 0, 100.00%, average loss 47.66958847045898\n",
            " Progress epoch 1, 20.00%, average loss 47.01358413696289\n",
            " Progress epoch 1, 40.00%, average loss 46.913551330566406\n",
            " Progress epoch 1, 60.00%, average loss 46.81439971923828\n",
            " Progress epoch 1, 80.00%, average loss 46.715935707092285\n",
            " Progress epoch 1, 100.00%, average loss 46.61785125732422\n",
            " Progress epoch 2, 20.00%, average loss 46.034915924072266\n",
            " Progress epoch 2, 40.00%, average loss 45.93830108642578\n",
            " Progress epoch 2, 60.00%, average loss 45.8430061340332\n",
            " Progress epoch 2, 80.00%, average loss 45.74771308898926\n",
            " Progress epoch 2, 100.00%, average loss 45.65342025756836\n",
            " Progress epoch 3, 20.00%, average loss 45.09093475341797\n",
            " Progress epoch 3, 40.00%, average loss 44.99818229675293\n",
            " Progress epoch 3, 60.00%, average loss 44.905296325683594\n",
            " Progress epoch 3, 80.00%, average loss 44.813241958618164\n",
            " Progress epoch 3, 100.00%, average loss 44.72074661254883\n",
            " Progress epoch 4, 20.00%, average loss 44.16220474243164\n",
            " Progress epoch 4, 40.00%, average loss 44.07406044006348\n",
            " Progress epoch 4, 60.00%, average loss 43.981667836507164\n",
            " Progress epoch 4, 80.00%, average loss 43.89076328277588\n",
            " Progress epoch 4, 100.00%, average loss 43.80146942138672\n",
            " Progress epoch 5, 20.00%, average loss 43.26399612426758\n",
            " Progress epoch 5, 40.00%, average loss 43.17572212219238\n",
            " Progress epoch 5, 60.00%, average loss 43.087869008382164\n",
            " Progress epoch 5, 80.00%, average loss 43.00021743774414\n",
            " Progress epoch 5, 100.00%, average loss 42.91406936645508\n",
            " Progress epoch 6, 20.00%, average loss 42.40076446533203\n",
            " Progress epoch 6, 40.00%, average loss 42.319467544555664\n",
            " Progress epoch 6, 60.00%, average loss 42.23926671346029\n",
            " Progress epoch 6, 80.00%, average loss 42.15916633605957\n",
            " Progress epoch 6, 100.00%, average loss 42.08143920898438\n",
            " Progress epoch 7, 20.00%, average loss 41.61975860595703\n",
            " Progress epoch 7, 40.00%, average loss 41.5504150390625\n",
            " Progress epoch 7, 60.00%, average loss 41.48177846272787\n",
            " Progress epoch 7, 80.00%, average loss 41.41241931915283\n",
            " Progress epoch 7, 100.00%, average loss 41.34542541503906\n",
            " Progress epoch 8, 20.00%, average loss 40.931304931640625\n",
            " Progress epoch 8, 40.00%, average loss 40.86673927307129\n",
            " Progress epoch 8, 60.00%, average loss 40.80487823486328\n",
            " Progress epoch 8, 80.00%, average loss 40.74239921569824\n",
            " Progress epoch 8, 100.00%, average loss 40.67754364013672\n",
            " Progress epoch 9, 20.00%, average loss 40.31402587890625\n",
            " Progress epoch 9, 40.00%, average loss 40.25778579711914\n",
            " Progress epoch 9, 60.00%, average loss 40.19930648803711\n",
            " Progress epoch 9, 80.00%, average loss 40.14192485809326\n",
            " Progress epoch 9, 100.00%, average loss 40.087572479248045\n",
            " Progress epoch 10, 20.00%, average loss 39.740516662597656\n",
            " Progress epoch 10, 40.00%, average loss 39.68135643005371\n",
            " Progress epoch 10, 60.00%, average loss 39.623156229654946\n",
            " Progress epoch 10, 80.00%, average loss 39.56718730926514\n",
            " Progress epoch 10, 100.00%, average loss 39.50992050170898\n",
            " Progress epoch 11, 20.00%, average loss 39.17174530029297\n",
            " Progress epoch 11, 40.00%, average loss 39.11927795410156\n",
            " Progress epoch 11, 60.00%, average loss 39.06763203938802\n",
            " Progress epoch 11, 80.00%, average loss 39.01528072357178\n",
            " Progress epoch 11, 100.00%, average loss 38.963070678710935\n",
            " Progress epoch 12, 20.00%, average loss 38.64555358886719\n",
            " Progress epoch 12, 40.00%, average loss 38.59145927429199\n",
            " Progress epoch 12, 60.00%, average loss 38.53397750854492\n",
            " Progress epoch 12, 80.00%, average loss 38.47954559326172\n",
            " Progress epoch 12, 100.00%, average loss 38.42335433959961\n",
            " Progress epoch 13, 20.00%, average loss 38.10942840576172\n",
            " Progress epoch 13, 40.00%, average loss 38.057369232177734\n",
            " Progress epoch 13, 60.00%, average loss 38.00560506184896\n",
            " Progress epoch 13, 80.00%, average loss 37.95695114135742\n",
            " Progress epoch 13, 100.00%, average loss 37.910630798339845\n",
            " Progress epoch 14, 20.00%, average loss 37.607757568359375\n",
            " Progress epoch 14, 40.00%, average loss 37.558841705322266\n",
            " Progress epoch 14, 60.00%, average loss 37.50942866007487\n",
            " Progress epoch 14, 80.00%, average loss 37.46307182312012\n",
            " Progress epoch 14, 100.00%, average loss 37.419969940185545\n",
            " Progress epoch 15, 20.00%, average loss 37.13227081298828\n",
            " Progress epoch 15, 40.00%, average loss 37.08601188659668\n",
            " Progress epoch 15, 60.00%, average loss 37.03424199422201\n",
            " Progress epoch 15, 80.00%, average loss 36.985350608825684\n",
            " Progress epoch 15, 100.00%, average loss 36.938009643554686\n",
            " Progress epoch 16, 20.00%, average loss 36.648223876953125\n",
            " Progress epoch 16, 40.00%, average loss 36.60710334777832\n",
            " Progress epoch 16, 60.00%, average loss 36.561622619628906\n",
            " Progress epoch 16, 80.00%, average loss 36.514495849609375\n",
            " Progress epoch 16, 100.00%, average loss 36.46689376831055\n",
            " Progress epoch 17, 20.00%, average loss 36.19554901123047\n",
            " Progress epoch 17, 40.00%, average loss 36.15468788146973\n",
            " Progress epoch 17, 60.00%, average loss 36.1000862121582\n",
            " Progress epoch 17, 80.00%, average loss 36.05657482147217\n",
            " Progress epoch 17, 100.00%, average loss 36.01429672241211\n",
            " Progress epoch 18, 20.00%, average loss 35.72821044921875\n",
            " Progress epoch 18, 40.00%, average loss 35.68393516540527\n",
            " Progress epoch 18, 60.00%, average loss 35.643751780192055\n",
            " Progress epoch 18, 80.00%, average loss 35.59881019592285\n",
            " Progress epoch 18, 100.00%, average loss 35.55160751342773\n",
            " Progress epoch 19, 20.00%, average loss 35.27475357055664\n",
            " Progress epoch 19, 40.00%, average loss 35.230634689331055\n",
            " Progress epoch 19, 60.00%, average loss 35.18856557210287\n",
            " Progress epoch 19, 80.00%, average loss 35.14618110656738\n",
            " Progress epoch 19, 100.00%, average loss 35.10069885253906\n",
            " Progress epoch 20, 20.00%, average loss 34.81931686401367\n",
            " Progress epoch 20, 40.00%, average loss 34.78808403015137\n",
            " Progress epoch 20, 60.00%, average loss 34.74032719930013\n",
            " Progress epoch 20, 80.00%, average loss 34.69612503051758\n",
            " Progress epoch 20, 100.00%, average loss 34.6457275390625\n",
            " Progress epoch 21, 20.00%, average loss 34.39630889892578\n",
            " Progress epoch 21, 40.00%, average loss 34.34117889404297\n",
            " Progress epoch 21, 60.00%, average loss 34.29306284586588\n",
            " Progress epoch 21, 80.00%, average loss 34.245057106018066\n",
            " Progress epoch 21, 100.00%, average loss 34.19734573364258\n",
            " Progress epoch 22, 20.00%, average loss 33.94779586791992\n",
            " Progress epoch 22, 40.00%, average loss 33.90875244140625\n",
            " Progress epoch 22, 60.00%, average loss 33.859822591145836\n",
            " Progress epoch 22, 80.00%, average loss 33.813361167907715\n",
            " Progress epoch 22, 100.00%, average loss 33.77816162109375\n",
            " Progress epoch 23, 20.00%, average loss 33.5196418762207\n",
            " Progress epoch 23, 40.00%, average loss 33.488094329833984\n",
            " Progress epoch 23, 60.00%, average loss 33.438733418782554\n",
            " Progress epoch 23, 80.00%, average loss 33.397212982177734\n",
            " Progress epoch 23, 100.00%, average loss 33.34694061279297\n",
            " Progress epoch 24, 20.00%, average loss 33.11820983886719\n",
            " Progress epoch 24, 40.00%, average loss 33.06977462768555\n",
            " Progress epoch 24, 60.00%, average loss 33.0331179300944\n",
            " Progress epoch 24, 80.00%, average loss 32.98991012573242\n",
            " Progress epoch 24, 100.00%, average loss 32.947661590576175\n",
            " Progress epoch 25, 20.00%, average loss 32.72257995605469\n",
            " Progress epoch 25, 40.00%, average loss 32.701181411743164\n",
            " Progress epoch 25, 60.00%, average loss 32.66592915852865\n",
            " Progress epoch 25, 80.00%, average loss 32.62405967712402\n",
            " Progress epoch 25, 100.00%, average loss 32.59882354736328\n",
            " Progress epoch 26, 20.00%, average loss 32.37041473388672\n",
            " Progress epoch 26, 40.00%, average loss 32.33476257324219\n",
            " Progress epoch 26, 60.00%, average loss 32.29943593343099\n",
            " Progress epoch 26, 80.00%, average loss 32.26324939727783\n",
            " Progress epoch 26, 100.00%, average loss 32.23432998657226\n",
            " Progress epoch 27, 20.00%, average loss 32.031734466552734\n",
            " Progress epoch 27, 40.00%, average loss 31.989924430847168\n",
            " Progress epoch 27, 60.00%, average loss 31.958764394124348\n",
            " Progress epoch 27, 80.00%, average loss 31.928368091583252\n",
            " Progress epoch 27, 100.00%, average loss 31.88678321838379\n",
            " Progress epoch 28, 20.00%, average loss 31.688987731933594\n",
            " Progress epoch 28, 40.00%, average loss 31.65469455718994\n",
            " Progress epoch 28, 60.00%, average loss 31.61226463317871\n",
            " Progress epoch 28, 80.00%, average loss 31.586020946502686\n",
            " Progress epoch 28, 100.00%, average loss 31.554689788818358\n",
            " Progress epoch 29, 20.00%, average loss 31.35645866394043\n",
            " Progress epoch 29, 40.00%, average loss 31.328993797302246\n",
            " Progress epoch 29, 60.00%, average loss 31.300785700480144\n",
            " Progress epoch 29, 80.00%, average loss 31.260162353515625\n",
            " Progress epoch 29, 100.00%, average loss 31.220363235473634\n",
            " Progress epoch 30, 20.00%, average loss 31.013565063476562\n",
            " Progress epoch 30, 40.00%, average loss 30.995418548583984\n",
            " Progress epoch 30, 60.00%, average loss 30.97820790608724\n",
            " Progress epoch 30, 80.00%, average loss 30.93948745727539\n",
            " Progress epoch 30, 100.00%, average loss 30.89935760498047\n",
            " Progress epoch 31, 20.00%, average loss 30.719436645507812\n",
            " Progress epoch 31, 40.00%, average loss 30.687382698059082\n",
            " Progress epoch 31, 60.00%, average loss 30.65021006266276\n",
            " Progress epoch 31, 80.00%, average loss 30.618958473205566\n",
            " Progress epoch 31, 100.00%, average loss 30.591297149658203\n",
            " Progress epoch 32, 20.00%, average loss 30.42222023010254\n",
            " Progress epoch 32, 40.00%, average loss 30.381738662719727\n",
            " Progress epoch 32, 60.00%, average loss 30.350844701131184\n",
            " Progress epoch 32, 80.00%, average loss 30.31734275817871\n",
            " Progress epoch 32, 100.00%, average loss 30.290945053100586\n",
            " Progress epoch 33, 20.00%, average loss 30.116012573242188\n",
            " Progress epoch 33, 40.00%, average loss 30.086191177368164\n",
            " Progress epoch 33, 60.00%, average loss 30.048612594604492\n",
            " Progress epoch 33, 80.00%, average loss 30.024529933929443\n",
            " Progress epoch 33, 100.00%, average loss 29.99080581665039\n",
            " Progress epoch 34, 20.00%, average loss 29.794857025146484\n",
            " Progress epoch 34, 40.00%, average loss 29.775626182556152\n",
            " Progress epoch 34, 60.00%, average loss 29.749998092651367\n",
            " Progress epoch 34, 80.00%, average loss 29.73757839202881\n",
            " Progress epoch 34, 100.00%, average loss 29.706196594238282\n",
            " Progress epoch 35, 20.00%, average loss 29.56136703491211\n",
            " Progress epoch 35, 40.00%, average loss 29.532371520996094\n",
            " Progress epoch 35, 60.00%, average loss 29.489848454793293\n",
            " Progress epoch 35, 80.00%, average loss 29.46605682373047\n",
            " Progress epoch 35, 100.00%, average loss 29.442188262939453\n",
            " Progress epoch 36, 20.00%, average loss 29.298730850219727\n",
            " Progress epoch 36, 40.00%, average loss 29.261432647705078\n",
            " Progress epoch 36, 60.00%, average loss 29.230946858723957\n",
            " Progress epoch 36, 80.00%, average loss 29.19631338119507\n",
            " Progress epoch 36, 100.00%, average loss 29.185182952880858\n",
            " Progress epoch 37, 20.00%, average loss 28.991308212280273\n",
            " Progress epoch 37, 40.00%, average loss 28.981196403503418\n",
            " Progress epoch 37, 60.00%, average loss 28.96054522196452\n",
            " Progress epoch 37, 80.00%, average loss 28.938270092010498\n",
            " Progress epoch 37, 100.00%, average loss 28.899776458740234\n",
            " Progress epoch 38, 20.00%, average loss 28.78766441345215\n",
            " Progress epoch 38, 40.00%, average loss 28.737866401672363\n",
            " Progress epoch 38, 60.00%, average loss 28.703702290852863\n",
            " Progress epoch 38, 80.00%, average loss 28.680942058563232\n",
            " Progress epoch 38, 100.00%, average loss 28.657904052734374\n",
            " Progress epoch 39, 20.00%, average loss 28.496036529541016\n",
            " Progress epoch 39, 40.00%, average loss 28.47467041015625\n",
            " Progress epoch 39, 60.00%, average loss 28.45576286315918\n",
            " Progress epoch 39, 80.00%, average loss 28.431861400604248\n",
            " Progress epoch 39, 100.00%, average loss 28.41136054992676\n",
            " Progress epoch 40, 20.00%, average loss 28.30802345275879\n",
            " Progress epoch 40, 40.00%, average loss 28.256596565246582\n",
            " Progress epoch 40, 60.00%, average loss 28.224560419718426\n",
            " Progress epoch 40, 80.00%, average loss 28.193029403686523\n",
            " Progress epoch 40, 100.00%, average loss 28.164142990112303\n",
            " Progress epoch 41, 20.00%, average loss 28.015914916992188\n",
            " Progress epoch 41, 40.00%, average loss 28.010683059692383\n",
            " Progress epoch 41, 60.00%, average loss 27.980254491170246\n",
            " Progress epoch 41, 80.00%, average loss 27.953736305236816\n",
            " Progress epoch 41, 100.00%, average loss 27.934263610839842\n",
            " Progress epoch 42, 20.00%, average loss 27.824535369873047\n",
            " Progress epoch 42, 40.00%, average loss 27.776698112487793\n",
            " Progress epoch 42, 60.00%, average loss 27.746700922648113\n",
            " Progress epoch 42, 80.00%, average loss 27.719275951385498\n",
            " Progress epoch 42, 100.00%, average loss 27.73146858215332\n",
            " Progress epoch 43, 20.00%, average loss 27.582033157348633\n",
            " Progress epoch 43, 40.00%, average loss 27.549221992492676\n",
            " Progress epoch 43, 60.00%, average loss 27.52875010172526\n",
            " Progress epoch 43, 80.00%, average loss 27.503894329071045\n",
            " Progress epoch 43, 100.00%, average loss 27.49039535522461\n",
            " Progress epoch 44, 20.00%, average loss 27.357309341430664\n",
            " Progress epoch 44, 40.00%, average loss 27.32683753967285\n",
            " Progress epoch 44, 60.00%, average loss 27.316145579020183\n",
            " Progress epoch 44, 80.00%, average loss 27.28898525238037\n",
            " Progress epoch 44, 100.00%, average loss 27.257411193847656\n",
            " Progress epoch 45, 20.00%, average loss 27.11222267150879\n",
            " Progress epoch 45, 40.00%, average loss 27.107922554016113\n",
            " Progress epoch 45, 60.00%, average loss 27.080764134724934\n",
            " Progress epoch 45, 80.00%, average loss 27.0687255859375\n",
            " Progress epoch 45, 100.00%, average loss 27.055181121826173\n",
            " Progress epoch 46, 20.00%, average loss 26.938884735107422\n",
            " Progress epoch 46, 40.00%, average loss 26.91191005706787\n",
            " Progress epoch 46, 60.00%, average loss 26.88961664835612\n",
            " Progress epoch 46, 80.00%, average loss 26.84946870803833\n",
            " Progress epoch 46, 100.00%, average loss 26.855373001098634\n",
            " Progress epoch 47, 20.00%, average loss 26.729015350341797\n",
            " Progress epoch 47, 40.00%, average loss 26.69011688232422\n",
            " Progress epoch 47, 60.00%, average loss 26.665788014729817\n",
            " Progress epoch 47, 80.00%, average loss 26.6422381401062\n",
            " Progress epoch 47, 100.00%, average loss 26.626894760131837\n",
            " Progress epoch 48, 20.00%, average loss 26.496183395385742\n",
            " Progress epoch 48, 40.00%, average loss 26.468082427978516\n",
            " Progress epoch 48, 60.00%, average loss 26.43922742207845\n",
            " Progress epoch 48, 80.00%, average loss 26.432079792022705\n",
            " Progress epoch 48, 100.00%, average loss 26.422777938842774\n",
            " Progress epoch 49, 20.00%, average loss 26.28557014465332\n",
            " Progress epoch 49, 40.00%, average loss 26.281129837036133\n",
            " Progress epoch 49, 60.00%, average loss 26.252464930216473\n",
            " Progress epoch 49, 80.00%, average loss 26.230856895446777\n",
            " Progress epoch 49, 100.00%, average loss 26.215844345092773\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  run_loss = 0.0\n",
        "\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "\n",
        "    run_loss += ret['loss'].item()\n",
        "\n",
        "    print('\\r Progress epoch {}, {:.2f}%, average loss {}'.format(epoch, (idx + 1) * 100.0 / len(train_iter), run_loss / (idx + 1.0))),"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d5UUnb13QYib"
      },
      "outputs": [],
      "source": [
        "test_set = MySet2(path_test)\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = batch_size,shuffle = False,pin_memory = True, collate_fn = collate_fn2)\n",
        "model.eval()\n",
        "for idx, data in enumerate(test_iter):\n",
        "  ret = model.run_on_batch(data, None)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "ogmrBN84SqF2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]"
      ],
      "metadata": {
        "id": "uf2tsKatEPel"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skdVIxrTa9Qf",
        "outputId": "ee6059a3-55d6-43e0-b450-2d5bc7fb843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN   206.0435028076172\n",
            "MEAN  249.26441955566406\n",
            "MEANP 259.15252685546875\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def calc_error_on_row(ret, index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1)#get all rows that are missing to compare to ground truth\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  return torch.abs(impu-real).mean()\n",
        "\n",
        "def calculate_personal_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean()\n",
        "\n",
        "def calc_mean_error(data,index_name):\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  filter = (forward['eval_masks'][:,:,index]==1)#get all rows that are missing\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  #mean_erro = torch.abs(real-z).mean()\n",
        "  mean_erro = torch.abs(real-z).mean()\n",
        "  return mean_erro\n",
        "\n",
        "index_name = 'calories'\n",
        "mean_erro = calc_mean_error(data,index_name)\n",
        "personal_mean_erro = calculate_personal_mean_error(data,index_name)\n",
        "rnn_erro = calc_error_on_row(ret,index_name)\n",
        "\n",
        "print(f\"RNN   {rnn_erro}\")\n",
        "print(f\"MEAN  {mean_erro}\")\n",
        "print(f\"MEANP {personal_mean_erro}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizations"
      ],
      "metadata": {
        "id": "X6Vi8cojJJKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use_norm = True\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "  \n",
        "\n",
        "in_file =  open(path_test,'rb')\n",
        "recs = pickle.load(in_file)\n",
        "in_file.close()\n",
        "#recs = [json.loads(x) for x in content]\n",
        "recs = [x['forward'] for x in recs]\n",
        "#recs = [[(y['values'][indexes['calories']] - normalizations['mean']['calories'] )/ normalizations['std']['calories'] for y in x]for x in recs]\n",
        "#recs = flatten(recs)\n",
        "#recs = [revert_norm(x,'calories') for x in recs]\n",
        "recs = [[revert_norm(y['values'][indexes['calories']],'calories')for y in x]for x in recs]\n",
        "recs = flatten(recs)\n",
        "recs = np.array(recs)\n",
        "recs.mean()"
      ],
      "metadata": {
        "id": "vk50FGc292jD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}