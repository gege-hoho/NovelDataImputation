{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b0RQ0wjd696j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX8U1J9QO8ak"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/imputation/* ."
      ],
      "metadata": {
        "id": "ZsmpVE9X68o9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRITS implementation in colab"
      ],
      "metadata": {
        "id": "bZvEXMNs5boo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AKL1BqLpOrry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wQnruihXPRPz"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 28\n",
        "RNN_HID_SIZE = 64\n",
        "INPUT_SIZE = 9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') \n",
        "    return device\n",
        "device = get_device()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPByV49cUi4-",
        "outputId": "3b5ff3b2-dec0-4d51-ffb3-fd2286cc0e44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Network Classes\n",
        "\n",
        "Other than in the original BRITS, we removed the classification loss, as we do not have a classificaition task. Further we did a hard split into train and test set as it is good practice. The code is based on, but updated, cleaned and converted to Python3:\n",
        "https://github.com/NIPS-BRITS/BRITS"
      ],
      "metadata": {
        "id": "sqTP34dWCt5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t0yxOHWNRE5q"
      },
      "outputs": [],
      "source": [
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.build(input_size)\n",
        "\n",
        "    def build(self, input_size):\n",
        "        self.W = Parameter(torch.Tensor(input_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(input_size)).to(device)\n",
        "\n",
        "        m = torch.ones(input_size, input_size) - torch.eye(input_size, input_size)\n",
        "        m = m.to(device)\n",
        "        self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_h = F.linear(x, self.W * self.m, self.b)\n",
        "        return z_h\n",
        "\n",
        "class TemporalDecay(nn.Module):\n",
        "    def __init__(self, input_size, output_size, diag = False):\n",
        "        super(TemporalDecay, self).__init__()\n",
        "        self.diag = diag\n",
        "\n",
        "        self.build(input_size, output_size)\n",
        "\n",
        "    def build(self, input_size, output_size):\n",
        "        self.W = Parameter(torch.Tensor(output_size, input_size)).to(device)\n",
        "        self.b = Parameter(torch.Tensor(output_size)).to(device)\n",
        "\n",
        "        if self.diag == True:\n",
        "            assert(input_size == output_size)\n",
        "            m = torch.eye(input_size, input_size).to(device)\n",
        "            self.register_buffer('m', m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(0))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        if self.b is not None:\n",
        "            self.b.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, d):\n",
        "        if self.diag == True:\n",
        "            gamma = F.relu(F.linear(d, self.W * self.m, self.b))\n",
        "        else:\n",
        "            gamma = F.relu(F.linear(d, self.W, self.b))\n",
        "        gamma = torch.exp(-gamma)\n",
        "        return gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uXh38LL8RF5P"
      },
      "outputs": [],
      "source": [
        "class RITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rnn_cell = nn.LSTMCell(INPUT_SIZE * 2, RNN_HID_SIZE).to(device)\n",
        "\n",
        "        self.temp_decay_h = TemporalDecay(input_size = INPUT_SIZE, output_size = RNN_HID_SIZE, diag = False)\n",
        "        self.temp_decay_x = TemporalDecay(input_size = INPUT_SIZE, output_size = INPUT_SIZE, diag = True)\n",
        "\n",
        "        self.hist_reg = nn.Linear(RNN_HID_SIZE, INPUT_SIZE).to(device)\n",
        "        self.feat_reg = FeatureRegression(INPUT_SIZE).to(device)\n",
        "\n",
        "        self.weight_combine = nn.Linear(INPUT_SIZE * 2, INPUT_SIZE).to(device)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.25).to(device)\n",
        "        self.out = nn.Linear(RNN_HID_SIZE, 1).to(device)\n",
        "\n",
        "    def forward(self, data, direct):\n",
        "        values = data[direct]['values'].to(device)\n",
        "        masks = data[direct]['masks'].to(device)\n",
        "        deltas = data[direct]['deltas'].to(device)\n",
        "\n",
        "        evals = data[direct]['evals'].to(device)\n",
        "        eval_masks = data[direct]['eval_masks'].to(device)\n",
        "\n",
        "        h = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "        c = torch.zeros((values.size()[0], RNN_HID_SIZE)).to(device)\n",
        "          \n",
        "\n",
        "        x_loss = 0.0\n",
        "        y_loss = 0.0\n",
        "\n",
        "        imputations = []\n",
        "\n",
        "        for t in range(SEQ_LEN):\n",
        "            x = values[:, t, :]\n",
        "            m = masks[:, t, :]\n",
        "            d = deltas[:, t, :]\n",
        "\n",
        "            gamma_h = self.temp_decay_h(d)\n",
        "            gamma_x = self.temp_decay_x(d)\n",
        "\n",
        "            h = h * gamma_h\n",
        "\n",
        "            x_h = self.hist_reg(h)\n",
        "            x_loss += torch.sum(torch.abs(x - x_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            x_c =  m * x +  (1 - m) * x_h\n",
        "\n",
        "            z_h = self.feat_reg(x_c)\n",
        "            x_loss += torch.sum(torch.abs(x - z_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            alpha = self.weight_combine(torch.cat([gamma_x, m], dim = 1))\n",
        "\n",
        "            c_h = alpha * z_h + (1 - alpha) * x_h\n",
        "            x_loss += torch.sum(torch.abs(x - c_h) * m) / (torch.sum(m) + 1e-5)\n",
        "\n",
        "            c_c = m * x + (1 - m) * c_h\n",
        "\n",
        "            inputs = torch.cat([c_c, m], dim = 1)\n",
        "\n",
        "            h, c = self.rnn_cell(inputs, (h, c))\n",
        "\n",
        "            imputations.append(c_c.unsqueeze(dim = 1))\n",
        "\n",
        "        imputations = torch.cat(imputations, dim = 1)\n",
        "\n",
        "        return {'loss': x_loss / SEQ_LEN,\n",
        "                'imputations': imputations,\n",
        "                'evals': evals, \n",
        "                'eval_masks': eval_masks}\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data, direct = 'forward')\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nqr5h57OQW93"
      },
      "outputs": [],
      "source": [
        "class BRITS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BRITS, self).__init__()\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        self.rits_f = RITS()\n",
        "        self.rits_b = RITS()\n",
        "\n",
        "    def forward(self, data):\n",
        "        ret_f = self.rits_f(data, 'forward')\n",
        "        ret_b = self.reverse(self.rits_b(data, 'backward'))\n",
        "\n",
        "        ret = self.merge_ret(ret_f, ret_b)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def merge_ret(self, ret_f, ret_b):\n",
        "        loss_f = ret_f['loss']\n",
        "        loss_b = ret_b['loss']\n",
        "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
        "\n",
        "        loss = loss_f + loss_b + loss_c\n",
        "\n",
        "        imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
        "\n",
        "        ret_f['loss'] = loss\n",
        "        ret_f['imputations'] = imputations\n",
        "\n",
        "        return ret_f\n",
        "\n",
        "    def get_consistency_loss(self, pred_f, pred_b):\n",
        "        #loss old:\n",
        "        #loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
        "        #return loss\n",
        "        loss = torch.abs(pred_f - pred_b).mean() * 1e-1\n",
        "        return loss\n",
        "\n",
        "    def reverse(self, ret):\n",
        "        def reverse_tensor(tensor_):\n",
        "            if tensor_.dim() <= 1:\n",
        "                return tensor_\n",
        "            indices = range(tensor_.size()[1])[::-1]\n",
        "            indices = torch.tensor(indices, requires_grad=False).long()#.requires_grad_(requires_grad=False) \n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                indices = indices.cuda()\n",
        "\n",
        "            return tensor_.index_select(1, indices)\n",
        "\n",
        "        for key in ret:\n",
        "            ret[key] = reverse_tensor(ret[key])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def run_on_batch(self, data, optimizer):\n",
        "        ret = self(data)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            ret['loss'].backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The Data is strucutred as follows:\n",
        "Each Entry in the Dataset is one Timeseries with n steps.\n",
        "It has a `forward` and a `backward` direction. For the both RITS Networks\n",
        "Each has following entries:\n",
        "\n",
        "*   `values`: data after elimination of values\n",
        "*   `masks`: indicating if data is missing\n",
        "*   `deltas`: timedeltas since last recorded data\n",
        "*   `evals`: ground truth\n",
        "*   `eval_masks`: 1 if is ground truth and missing in values 0 otherwise"
      ],
      "metadata": {
        "id": "gnRiD5GztpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict_entries=['values','masks','deltas','evals','eval_masks']\n",
        "\n",
        "class MySet2(Dataset):\n",
        "    def __init__(self,content_path):\n",
        "        super(MySet2, self).__init__()\n",
        "        content = open(content_path,'rb')\n",
        "        recs = pickle.load(content)\n",
        "        content.close()\n",
        "        self.forward = self.to_tensor_dict([x['forward'] for x in recs])\n",
        "        self.backward = self.to_tensor_dict([x['backward'] for x in recs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.forward[data_set_dict_entries[0]])\n",
        "    \n",
        "    def to_tensor_dict(self,recs):\n",
        "      return_dict = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        tens = torch.FloatTensor([[x[dict_key][0:INPUT_SIZE]for x in r]for r in recs])\n",
        "        return_dict[dict_key] = tens \n",
        "      return return_dict\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      forward = {}\n",
        "      backward = {}\n",
        "      for dict_key in data_set_dict_entries:\n",
        "        forward[dict_key] = self.forward[dict_key][idx]\n",
        "        backward[dict_key] = self.backward[dict_key][idx]\n",
        "      return {'forward':forward,'backward':backward}\n",
        "\n",
        "def collate_fn2(recs):\n",
        "  batch_size = len(recs)\n",
        "  forward = {}\n",
        "  backward = {}\n",
        "  for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "      backward[dict_key] = torch.empty(batch_size,SEQ_LEN,INPUT_SIZE)\n",
        "  for idx,x in enumerate(recs):\n",
        "    for dict_key in data_set_dict_entries:\n",
        "      forward[dict_key][idx] = x['forward'][dict_key]\n",
        "      backward[dict_key][idx] = x['backward'][dict_key] \n",
        "  return {'forward': forward, 'backward': backward}"
      ],
      "metadata": {
        "id": "syj7aGr9qeiR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Functions"
      ],
      "metadata": {
        "id": "gTlYptSrDaqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "skdVIxrTa9Qf"
      },
      "outputs": [],
      "source": [
        "indexes= {'calories':1,'carbs':2,'fat':3,'protein':4}\n",
        "def load_normalizations(file):\n",
        "  content = open(file,'rb')\n",
        "  ret = pickle.load(content)\n",
        "  content.close()\n",
        "  return ret\n",
        "normalizations = load_normalizations('brits_normalization.pickle')\n",
        "\n",
        "def revert_norm(value,name):\n",
        "  if not use_norm:\n",
        "    #only if we actually use normalized data\n",
        "    return value\n",
        "  return value * normalizations['std'][name] + normalizations['mean'][name]\n",
        "\n",
        "def get_missing_and_index(index_name):\n",
        "  index = indexes[index_name]\n",
        "  filter = (ret['eval_masks'][:,:,index]==1) #get all rows that are missing to compare to ground truth\n",
        "  return index,filter\n",
        "\n",
        "def print_impu_real(ret,index_name):\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  px = pd.DataFrame()\n",
        "  px.insert(0,\"Real\",real.numpy())\n",
        "  px.insert(0,\"Imputation\",impu.detach().numpy())\n",
        "  return px\n",
        "\n",
        "def get_abs_error_val(ret, index_name):\n",
        "  \"\"\"Calculates the absolute error of the brits imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = revert_norm(ret['imputations'][:,:,index][filter],index_name)\n",
        "  real = revert_norm(ret['evals'][:,:,index][filter],index_name)\n",
        "  return torch.abs(impu-real).mean().item()\n",
        "\n",
        "def get_abs_personal_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the personal mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index = indexes[index_name]\n",
        "  forward = data['forward']\n",
        "  error = torch.tensor(())\n",
        "  for x in range(forward['eval_masks'].size(dim=0)):\n",
        "    filter = (forward['eval_masks'][x,:,index]==1)#get all rows that are missing\n",
        "    z =  revert_norm(forward['values'][x,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "    real =  revert_norm(forward['evals'][x,:,index][filter],index_name)\n",
        "    error = torch.cat((error,torch.abs(real-z)), dim=0)\n",
        "  return error.mean().item()\n",
        "\n",
        "def get_abs_mean_impu_error(data,index_name):\n",
        "  \"\"\"Calculates the absolute error of the mean imputation for each missing value and returns the mean error\"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  forward = data['forward']\n",
        "  z =  revert_norm(forward['values'][:,:,index][filter==False],index_name).mean()#calculate the mean of all non missing\n",
        "  real =  revert_norm(forward['evals'][:,:,index][filter],index_name)\n",
        "  mean_erro = torch.abs(real-z).mean().item()\n",
        "  return mean_erro\n",
        "\n",
        "def get_rel_error_val(ret,index_name):\n",
        "  \"\"\"Calculates the relative error of the brits imputation for each missing value and returns the mean error\n",
        "  It uses the normalized values to remove problems with 0 division\n",
        "  \"\"\"\n",
        "  index, filter = get_missing_and_index(index_name)\n",
        "  impu = ret['imputations'][:,:,index][filter]\n",
        "  real = ret['evals'][:,:,index][filter]\n",
        "  return torch.abs((impu-real)/(torch.abs(real)+1)).mean().item()\n",
        "\n",
        "def get_errors_impu(ret):\n",
        "  return_dict = {}\n",
        "  for index_name in indexes.keys():\n",
        "    #rel_err = get_rel_error_val(ret,index_name)\n",
        "    abs_err = get_abs_error_val(ret, index_name)\n",
        "    return_dict[index_name] = abs_err\n",
        "  return return_dict\n",
        "\n",
        "def get_errors_bench(data):\n",
        "  ret = {}\n",
        "  for index_name in indexes.keys():\n",
        "    abs_err = get_abs_mean_impu_error(data,index_name)\n",
        "    ret[index_name] = abs_err\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "R8GWstVcDeEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_norm = True\n",
        "path_train = './brits_train.pickle'\n",
        "path_test = './brits_test.pickle'\n",
        "if not use_norm:\n",
        "  path_train = './brits_train_nonnorm.pickle'\n",
        "  path_test = './brits_test_nonnorm.pickle'\n",
        "test_set = MySet2(path_test)\n",
        "train_set = MySet2(path_train)\n",
        "\n",
        "if len(test_set) == len(train_set):\n",
        "  raise Exception(\"Length of test and train set are equal. Is there a Mistake?\")"
      ],
      "metadata": {
        "id": "xW5WF4euqutu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    lr = 1e-3,\n",
        "    batch_size= 500,\n",
        "    use_norm = use_norm,\n",
        "    num_workers=1,\n",
        "    dataset=\"My Fitnesspal Small\",\n",
        "    epochs=4000,\n",
        "    test_set_size= len(test_set),\n",
        "    train_set_size= len(train_set),\n",
        "    early_stop_after=2\n",
        ")"
      ],
      "metadata": {
        "id": "1BroW_JYqPZ5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "run = wandb.init(project=\"brits\", entity=\"gege-hoho\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mUi5hi87meuW",
        "outputId": "ccc6eecd-7888-4de0-d723-aea8e1cbcab3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgege-hoho\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/gege-hoho/brits/runs/a6u211un\" target=\"_blank\">pleasant-eon-6</a></strong> to <a href=\"https://wandb.ai/gege-hoho/brits\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EgS9qpDgVke_"
      },
      "outputs": [],
      "source": [
        "model = BRITS()\n",
        "optimizer = optim.Adam(model.parameters(), lr = config['lr'] )\n",
        "\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = config['batch_size'],\n",
        "                        shuffle = True,pin_memory = True, \n",
        "                        collate_fn = collate_fn2, \n",
        "                        num_workers=config[\"num_workers\"])\n",
        "\n",
        "test_iter = DataLoader(dataset = test_set,batch_size = config['batch_size'],shuffle = False,pin_memory = True, collate_fn = collate_fn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RxB3iYgjRfGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ecbe5d-e216-4eac-c197-9e6f9c46b33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [25:16<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "cuda:0 1516.259934425354\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "last_errors = {}\n",
        "for epoch in tqdm(range(config['epochs'])):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for idx, data in enumerate(train_iter):\n",
        "    ret = model.run_on_batch(data, optimizer)\n",
        "    train_loss += ret['loss'].item()\n",
        "  train_loss = train_loss/(idx + 1.0)\n",
        "  wandb.log({\"train-loss\": train_loss})\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  for idx,data in enumerate(test_iter):\n",
        "    ret = model.run_on_batch(data, None)\n",
        "    test_loss += ret['loss'].item()\n",
        "  test_loss = test_loss/(idx+ 1.0)\n",
        "  test_errors = get_errors_impu(ret)\n",
        "  wandb.log({\"test-loss\": test_loss})\n",
        "  wandb.log({\"test-errors\": test_errors})\n",
        "\n",
        "  #early stopping\n",
        "  count = sum([1 for k,v in last_errors.items() if v < test_errors[k]])\n",
        "  if count >= 2:\n",
        "    #more than 2 values went worse so we stop\n",
        "    print(\"\\n Early Stopping\")\n",
        "    break\n",
        "print(\"\\n\")\n",
        "print(f\"{device} {time.time()-t0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing against MEAN"
      ],
      "metadata": {
        "id": "7xIYPQNQSGhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for idx, data in enumerate(train_iter):\n",
        "  ret = model.run_on_batch(data, optimizer)\n",
        "  break\n",
        "index_name = 'calories'\n",
        "mean_erro = get_abs_mean_impu_error(data,index_name)\n",
        "personal_mean_erro = get_abs_personal_mean_impu_error(data,index_name)\n",
        "rnn_erro = get_abs_error_val(ret,index_name)\n",
        "\n",
        "print(f\"RNN   {rnn_erro}\")\n",
        "print(f\"MEAN  {mean_erro}\")\n",
        "print(f\"MEANP {personal_mean_erro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w4-NcN_tjf4",
        "outputId": "06dd5eaa-ec52-462b-fae6-f2c7797f1401"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN   393.9133605957031\n",
            "MEAN  248.78421020507812\n",
            "MEANP 257.0036926269531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "use_norm = False\n",
        "path_train = './brits_train.pickle'\n",
        "path_train2 = './brits_train_nonnorm.pickle'\n",
        "train_set = MySet2(path_train)\n",
        "train_set2 = MySet2(path_train2)\n",
        "train_iter = DataLoader(dataset = train_set,batch_size = 5000,\n",
        "                        shuffle = False,pin_memory = True, \n",
        "                        collate_fn = collate_fn2, \n",
        "                        num_workers=config[\"num_workers\"])\n",
        "train_iter2 = DataLoader(dataset = train_set2,batch_size = 5000,\n",
        "                        shuffle = False,pin_memory = True, \n",
        "                        collate_fn = collate_fn2, \n",
        "                        num_workers=config[\"num_workers\"])\n",
        "\n",
        "for d in train_iter:\n",
        "  break\n",
        "for d2 in train_iter2:\n",
        "  break\n",
        "d2 = d2['forward']\n",
        "d = d['forward']\n",
        "d2 = d2['values'][:,:,1].view(8288)\n",
        "d = d['values'][:,:,1].view(8288)\n",
        "dnorm = revert_norm(d,'calories').numpy()\n",
        "print(dnorm.mean())\n",
        "print(d2.mean().item())\n",
        "px = pd.DataFrame()\n",
        "px[\"d\"] = d.numpy()\n",
        "px[\"dnorm\"] = dnorm\n",
        "px[\"d2\"] = d2.numpy()\n",
        "px[\"d-d2\"] = px[\"dnorm\"]-px[\"d2\"]\n",
        "px[(px[\"d-d2\"] >=0.1)]\n",
        "#px\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "5WxfEEr0Dv0w",
        "outputId": "203b676c-1362-496e-896e-258529227524"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.861616\n",
            "456.36041259765625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c87227b6-3f1a-46c9-9a36-37cfe02286dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d</th>\n",
              "      <th>dnorm</th>\n",
              "      <th>d2</th>\n",
              "      <th>d-d2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c87227b6-3f1a-46c9-9a36-37cfe02286dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c87227b6-3f1a-46c9-9a36-37cfe02286dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c87227b6-3f1a-46c9-9a36-37cfe02286dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [d, dnorm, d2, d-d2]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BRITS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}