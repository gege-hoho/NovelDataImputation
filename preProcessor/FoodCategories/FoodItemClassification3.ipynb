{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FoodItemClassification3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZIqEvxtVZgo"
      },
      "source": [
        "Version 3 of FoodItemClassification based on CNN by Kim https://arxiv.org/abs/1408.5882\n",
        "\n",
        "FoodItem2.1: Has 4 filters (1,3,4,5) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9_3UXL6oAU9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set createData to true, to extract train and test data from usda dataset. Otherwise, the code expects two pickles at\n",
        "* `data/dataset_train`\n",
        "* `data/dataset_test`"
      ],
      "metadata": {
        "id": "W83bNkLAD91I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "createData = False"
      ],
      "metadata": {
        "id": "qXwGr8zwD8xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy model data (embedding model and bigram model) created by `WordEmbeddingModel.ipynb`"
      ],
      "metadata": {
        "id": "c0roLXh6EU4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/models\n",
        "!cp drive/MyDrive/Uni/Masterarbeit/data/embeddingmodel.zip data/models/zip.zip\n",
        "!unzip -o data/models/zip.zip -d  data/models "
      ],
      "metadata": {
        "id": "6LhOeDPoEUTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Copy train dataset from google drive (if not available set `createData=True` to create)\n",
        "* Copy test dataset from google drive (if not available set `createData=True` to create)\n",
        "* Copy a selection from the crawled data from google drive (use `misc/export_data.py` for export)"
      ],
      "metadata": {
        "id": "6bNbsZ73EfTT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z8nvUpB-NiI"
      },
      "source": [
        "!cp drive/MyDrive/Uni/Masterarbeit/data/dataset_train data/dataset_train\n",
        "!cp drive/MyDrive/Uni/Masterarbeit/data/dataset_test data/dataset_test\n",
        "!cp drive/MyDrive/Uni/Masterarbeit/data/crawled.csv data/crawled.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foVIvMDEJ1VG"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYaeEhgG-RTj"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import time\n",
        "import urllib.request\n",
        "import os.path\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define mapping dict to reduce the number of categories, all categories in the list are getting mapped to the dict key"
      ],
      "metadata": {
        "id": "X4ivny3sHdcZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-MUQIrjpg_Y"
      },
      "source": [
        "mapping = {'Baking':['Baking Additives & Extracts','Baking Decorations & Dessert Toppings','Baking/Cooking Mixes/Supplies'],\n",
        "'Biscuits/Cookies':['Biscuits/Cookies','Biscuits/Cookies (Shelf Stable)'],\n",
        "'Bread':['Bread','Bread & Muffin Mixes','Breads & Buns'],\n",
        "'Cakes':['Cake, Cookie & Cupcake Mixes','Cakes - Sweet (Frozen)','Cakes, Cupcakes, Snack Cakes','Croissants, Sweet Rolls, Muffins & Other Pastries','Pies/Pastries - Sweet (Shelf Stable)'],\n",
        "'Soup':['Canned Soup','Canned Condensed Soup','Other Soups','Chili & Stew'],\n",
        "'Cereal':['Cereal','Cereals Products - Ready to Eat (Shelf Stable)','Processed Cereal Products'],\n",
        "'Chips, Pretzels & Snacks': ['Chips, Pretzels & Snacks','Chips/Crisps/Snack Mixes - Natural/Extruded (Shelf Stable)','Popcorn, Peanuts, Seeds & Related Snacks','Wholesome Snacks','Crackers & Biscotti','Flavored Snack Crackers','Other Snacks','Snacks'],\n",
        "'Fish & Seafood': ['Fish  Unprepared/Unprocessed','Fish & Seafood','Fish – Unprepared/Unprocessed','Canned Seafood','Canned Tuna','Frozen Fish & Seafood','Shellfish Unprepared/Unprocessed'],\n",
        "'Chocolate':['Confectionery Products','Chocolate'],\n",
        "'Oils & Butters':['Oils Edible','Nut & Seed Butters','Butter & Spread','Vegetable & Cooking Oils'],\n",
        "'Dough Based Products':['Dough Based Products / Meals','Frozen Bread & Dough'],\n",
        "'Bacon, Sausages & Ribs':['Frozen Bacon, Sausages & Ribs','Frozen Sausages, Hotdogs & Brats','Bacon, Sausages & Ribs','Sausages, Hotdogs & Brats'],\n",
        "'Flours & Grains':['Flours & Corn Meal','Grains/Flour'],\n",
        "'Herbs & Spices':['Herbs & Spices','Herbs/Spices/Extracts'],\n",
        "'Meat/Poultry/Other Animals':['Meat/Poultry/Other Animals  Prepared/Processed','Meat/Poultry/Other Animals  Unprepared/Unprocessed',\n",
        "                              'Meat/Poultry/Other Animals Sausages  Prepared/Processed','Meat/Poultry/Other Animals Sausages – Prepared/Processed',\n",
        "                              'Meat/Poultry/Other Animals – Prepared/Processed','Meat/Poultry/Other Animals – Unprepared/Unprocessed',\n",
        "                              'Other Frozen Meats','Poultry, Chicken & Turkey','Canned Meat','Frozen Poultry, Chicken & Turkey','Other Meats','Frozen Patties and Burgers'],\n",
        "'Non Alcoholic Beverages':['Non Alcoholic Beverages  Not Ready to Drink','Non Alcoholic Beverages  Ready to Drink','Non Alcoholic Beverages – Ready to Drink'],\n",
        "'Cooking Sauces':['Cooking Sauces','Oriental, Mexican & Ethnic Sauces','Other Cooking Sauces'],\n",
        "'Pasta/Noodles':['Pasta by Shape & Type','Pasta/Noodles','All Noodles'],\n",
        "'Subs, Sandwiches, Wraps & Burittos':['Prepared Wraps and Burittos','Prepared Subs & Sandwiches'],\n",
        "'Vegetables':['Vegetables  Prepared/Processed','Vegetables – Prepared/Processed','Canned Vegetables','Tomatoes','Frozen Vegetables'],\n",
        "'Vegetable Based Products / Meals':['Vegetable Based Products / Meals','Vegetable Based Products / Meals - Not Ready to Eat (Frozen)'],\n",
        "'Pancakes, Waffles, French Toast & Crepes':['Frozen Pancakes, Waffles, French Toast & Crepes', 'Pancakes, Waffles, French Toast & Crepes'],\n",
        "'Entrees, Sides & Small Meals': ['Entrees, Sides & Small Meals',\"Frozen Appetizers & Hors D'oeuvres\",'Frozen Prepared Sides'],\n",
        "'Fruit  Prepared/Processed':['Fruit  Prepared/Processed','Fruit - Prepared/Processed (Shelf Stable)']}\n",
        "remove = [\"Baby/Infant  Foods/Beverages\",\"Miscellanious\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions later used:\n",
        "\n",
        "\n",
        "*   `get_device()`: returns cuda if available otherwise cpu\n",
        "*   `unify_cat(dataframe, col, cat_list,new_cat)`: put all categories in `cat_list` into `new_cat` for a given pandas `dataframe`\n",
        "*   `get_data(file_path)`: reads in the original usda csv data branded_food.csv  and food.csv\n",
        "*   `get_splitted_data(file_path, stratify=True, max_data= -1)`: reads in the original usda csv data and splits into train and test set, with `stratify` one can decide to stratify the data and one can limit the amount of extracted data with `max_data`\n",
        "*   `early_stopping(early_stop_after)`: returns a function that decides if early stopping should be applied based on `early_stop_after`\n",
        "*   `test_crawled(curr_model,categories)`: returns dataframe that gives an overview about the categories for the MyFitnessPal data in `crawled.csv`"
      ],
      "metadata": {
        "id": "aOahIAiYH7MV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh_UW-HcQGOy"
      },
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') \n",
        "    return device\n",
        "  \n",
        "def unify_cat(dataframe, col, cat_list,new_cat):\n",
        "  #unify several categories into 1 with new name\n",
        "  if new_cat not in dataframe[col].cat.categories:\n",
        "    dataframe[col].cat.add_categories(new_cat,inplace = True)\n",
        "  dataframe.loc[dataframe[col].isin(cat_list),col] = new_cat\n",
        "  dataframe[col].cat.remove_unused_categories(inplace=True)\n",
        "\n",
        "def get_data(file_path):\n",
        "  branded_food = pd.read_csv(file_path[0])\n",
        "  food2 = pd.read_csv(file_path[1])\n",
        "  food2 = branded_food.merge(food2, on='fdc_id')\n",
        "  del branded_food\n",
        "  food = pd.DataFrame(food2[\"description\"])\n",
        "  food[\"brand\"] = food2[\"brand_name\"].fillna(\"\").astype(str)\n",
        "  food[\"category\"] = food2[\"branded_food_category\"].astype('category')\n",
        "  del food2\n",
        "  vc = food[\"category\"].value_counts()\n",
        "  vc = vc[vc > 100]\n",
        "  no_of_classes = len(vc)\n",
        "  food = food[food[\"category\"].isin(vc.index)]\n",
        "  food[\"category\"] = food[\"category\"].cat.remove_unused_categories()\n",
        "  food[\"description\"] = food[\"description\"].fillna(\"\").astype(str)\n",
        "  food = food[~food[\"category\"].isnull()]\n",
        "  #put similar stuff together\n",
        "  for x in remove:\n",
        "    food = food[food[\"category\"] != x]\n",
        "  for x,y in mapping.items():\n",
        "    unify_cat(food,\"category\",y,x)\n",
        "  food[\"category\"] = food[\"category\"].cat.remove_unused_categories()\n",
        "  return food\n",
        "\n",
        "def get_splitted_data(file_path, stratify=True, max_data= -1):\n",
        "  data = get_data(file_path)\n",
        "  if max_data != -1:\n",
        "    data = data.sample(max_data)\n",
        "  if stratify:\n",
        "    return train_test_split(data,test_size=0.2,stratify=data[\"category\"])\n",
        "  return train_test_split(data,test_size=0.2)\n",
        "\n",
        "def early_stopping(early_stop_after):\n",
        "  early_stop_counter=0\n",
        "  best_early_stopping = 0\n",
        "  def inner_early_stopping(current):\n",
        "    nonlocal early_stop_counter, best_early_stopping\n",
        "    if best_early_stopping > current:\n",
        "      early_stop_counter +=1\n",
        "    else:\n",
        "      best_early_stopping = current\n",
        "      early_stop_counter =0\n",
        "    return early_stop_counter >= early_stop_after\n",
        "\n",
        "  return inner_early_stopping\n",
        "\n",
        "def test_crawled(curr_model,categories):\n",
        "  categories = np.array(categories)\n",
        "  crawled_food = pd.read_csv(\"data/crawled.csv\")\n",
        "  crawled_food[\"tokens\"] = crawled_food.apply(lambda row: bigram_model[fd.preprocess(row[\"meal_name\"])], axis=1)\n",
        "  crawled_x = [embedd(v) for _,v in crawled_food[\"tokens\"].iteritems()]\n",
        "  crawled_x = torch.stack(crawled_x).float().to(device)\n",
        "  curr_model.eval()\n",
        "  y = curr_model(crawled_x)\n",
        "  y = torch.nn.functional.softmax(y,dim=1)\n",
        "  y = y.cpu()\n",
        "  val,idx = torch.topk(y, 1, dim=1)\n",
        "  crawled_food[\"category\"] = categories[np.array(idx[:,0])]\n",
        "  crawled_food[\"pct\"] = val[:,0].detach().numpy()\n",
        "  return crawled_food"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the embedding and bigram model"
      ],
      "metadata": {
        "id": "PuVcPBBEHyXr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2hnaqI0QKwB"
      },
      "source": [
        "embedding_model = KeyedVectors.load(\"data/models/mymodel\")\n",
        "embedding_size = 300\n",
        "bigram_model = Phrases.load(\"data/models/bigram_model.pkl\")\n",
        "bigram_model = Phraser(bigram_model)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "custom = [\"gal\", \"oz\", \"t\", \"tsp\", \"teaspoon\", \n",
        "          \"tablespoon\", \"tbl\", \"tbs\", \"tbsp\",\n",
        "          \"fl\", \"oz\", \"gil\", \"ounce\", \"ml\", \"l\",\n",
        "          \"dl\", \"lb\", \"pund\", \"mg\", \"g\", \"kg\", \"gram\", \"cup\",\"cups\",\"container\",\"avg\",\"homemade\",\"piece\",\"serving\",\"spam\",\"servings\",\"grams\"]\n",
        "lst_stopwords.extend(custom)\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Create the USDA dataset class"
      ],
      "metadata": {
        "id": "RLRELa4MLryJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FCGgEfpQCW4"
      },
      "source": [
        "class FoodItemDataset(Dataset):\n",
        "    def __init__(self, data, token_count,device):\n",
        "      self.token_count = token_count\n",
        "      self.no_of_classes = len(data[\"category\"].value_counts())\n",
        "      self.codes = list(data[\"category\"].cat.codes)\n",
        "      self.cats = list(data[\"category\"].cat.categories)\n",
        "      self.embedding = [self.calc(i,row[1]) for i,row in enumerate(data.iterrows())]\n",
        "      \n",
        "      \n",
        "      self.codes = np.array(self.codes, dtype=np.int_)\n",
        "    def __len__(self):\n",
        "      return len(self.embedding)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return (self.codes[idx],self.embedding[idx])\n",
        "\n",
        "    def preprocess(self,name):\n",
        "      name = re.sub(r'[^\\w\\s]', '', str(name).lower().strip())\n",
        "      name = tokenizer.tokenize(name)\n",
        "      return [lemmatizer.lemmatize(x) for x in name if x not in lst_stopwords and len(x)>2 and not any(char.isdigit() for char in x)]\n",
        "\n",
        "    def calc(self, index, row):\n",
        "      if index%100000 == 0:\n",
        "        print(index)\n",
        "      embedding = np.zeros((self.token_count,embedding_size),dtype=float)\n",
        "      tokens_brand = self.preprocess(row[\"brand\"])\n",
        "      tokens = self.preprocess(row[\"description\"])\n",
        "      #if not any(x in tokens for x in tokens_brand):\n",
        "      tokens = tokens_brand + tokens\n",
        "      tokens = bigram_model[tokens]\n",
        "      return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the data. if `createData` is True then download it from usda and fill dataset class (could take a while). Otherwise load the Dataset from pickle"
      ],
      "metadata": {
        "id": "A5iCtNZHLyU4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgWffI_WEPGX"
      },
      "source": [
        "device = get_device()\n",
        "if createData:\n",
        "  if os.path.exists(\"zip.zip\"):\n",
        "    assert False\n",
        "  urllib.request.urlretrieve(\"https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-10-28.zip\", \"zip.zip\")\n",
        "  !unzip zip.zip -d data\n",
        "  data_train, data_test = get_splitted_data((\"data/branded_food.csv\",\"data/food.csv\"))\n",
        "  fd = FoodItemDataset(data_train,10,device)\n",
        "  fd_test = FoodItemDataset(data_test,10,device)\n",
        "  torch.save(fd,\"data/dataset_train\")\n",
        "  torch.save(fd_test,\"data/dataset_test\")\n",
        "else:\n",
        "  fd = torch.load(\"data/dataset_train\")\n",
        "  fd_test = torch.load(\"data/dataset_test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the collate_batch used in the Dataloader"
      ],
      "metadata": {
        "id": "XWnzbFC5TR1z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmBhXdzeFly"
      },
      "source": [
        "def embedd(tokens):\n",
        "  embedding = np.zeros((10,embedding_size),dtype=float)\n",
        "  for i,token in enumerate(tokens[:10]):\n",
        "    if not(token == \"\" or token == \" \" or token not in embedding_model.wv):\n",
        "      embedding[i,:] = embedding_model.wv[token]\n",
        "  return torch.FloatTensor(embedding)\n",
        "\n",
        "def embedd_mean(tokens):\n",
        "  i = 0\n",
        "  embedding = np.zeros((embedding_size,),dtype=float)\n",
        "  for token in tokens[:10]:\n",
        "    if not(token == \"\" or token == \" \" or token not in embedding_model.wv):\n",
        "      embedding += embedding_model.wv[token]\n",
        "      i += 1\n",
        "  if i == 0:\n",
        "    return torch.FloatTensor(embedding)\n",
        "  return torch.FloatTensor(embedding/i)\n",
        "\n",
        "def collate_batch(batch):\n",
        "  code_list, embedding_list = [],[]\n",
        "  for code, tokens in batch:\n",
        "    code_list.append(code)\n",
        "    embedding_list.append(embedd(tokens))\n",
        "  code_list = torch.LongTensor(code_list)\n",
        "  embedding_list = torch.stack(embedding_list)\n",
        "  return code_list, embedding_list\n",
        "\n",
        "def collate_batch_eval(batch):\n",
        "  code_list, embedding_list = collate_batch(batch)\n",
        "  tokens_list = [x for _,x in batch]\n",
        "  return code_list,embedding_list, tokens_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuCEdJBM5geT"
      },
      "source": [
        "Define the FoodClassificationNetwork with 4 filter sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSu_KlpAycDK"
      },
      "source": [
        "class FoodClassificationCnnModel(Module):\n",
        "  def __init__(self, no_of_classes,device,dropout=0.2):\n",
        "    super().__init__()\n",
        "    n_filters = 100\n",
        "    self.device = device\n",
        "    self.no_of_classes = no_of_classes\n",
        "    self.filter1 =  torch.nn.Conv2d(in_channels = 1, \n",
        "                                    out_channels = n_filters, \n",
        "                                    kernel_size = (3, 300)).to(device)\n",
        "    self.filter2 =  torch.nn.Conv2d(in_channels = 1, \n",
        "                                    out_channels = n_filters, \n",
        "                                    kernel_size = (4, 300)).to(device) \n",
        "    self.filter3 =  torch.nn.Conv2d(in_channels = 1, \n",
        "                                    out_channels = n_filters, \n",
        "                                    kernel_size = (5, 300)).to(device)\n",
        "    self.filter4 =  torch.nn.Conv2d(in_channels = 1, \n",
        "                                    out_channels = n_filters, \n",
        "                                    kernel_size = (1, 300)).to(device)\n",
        "    self.linear = torch.nn.Sequential(torch.nn.Linear(4*n_filters,100),\n",
        "                                      torch.nn.LeakyReLU(),\n",
        "                                      torch.nn.BatchNorm1d(100),\n",
        "                                      torch.nn.Linear(100,self.no_of_classes)).to(device)\n",
        "    self.relu = torch.nn.ReLU().to(device)\n",
        "    self.dropout = torch.nn.Dropout(dropout).to(device)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = x.unsqueeze(1)\n",
        "    f1 = self.relu(self.filter1(x)).squeeze(3)\n",
        "    f2 = self.relu(self.filter2(x)).squeeze(3)\n",
        "    f3 = self.relu(self.filter3(x)).squeeze(3)\n",
        "    f4 = self.relu(self.filter4(x)).squeeze(3)\n",
        "    f1 = torch.nn.functional.max_pool1d(f1,f1.shape[2])\n",
        "    f2 = torch.nn.functional.max_pool1d(f2,f2.shape[2])\n",
        "    f3 = torch.nn.functional.max_pool1d(f3,f3.shape[2])\n",
        "    f4 = torch.nn.functional.max_pool1d(f4,f4.shape[2])\n",
        "    linear = self.dropout(torch.cat((f1,f2,f3,f4),dim=1)).squeeze(2)\n",
        "    out = self.linear(linear)\n",
        "    return out\n",
        "    \n",
        "  def get_accuracy(self,X,y):\n",
        "    y_pred = self.forward(X)\n",
        "    res = torch.argmax(y_pred, dim=1)\n",
        "    res = y-res\n",
        "    l = int(torch.count_nonzero(res))\n",
        "    count = list(y_pred.shape)[0]\n",
        "    return (count-l)/count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8t99AwpLutB"
      },
      "source": [
        "config = dict(\n",
        "    lr = 1e-4,\n",
        "    no_of_classes = fd.no_of_classes,\n",
        "    batch_size= 50000,\n",
        "    num_workers=2,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    dataset=\"FoodItem2.1(1,300)Filter\",\n",
        "    epochs=30,\n",
        "    test_set_size= len(fd_test),\n",
        "    train_set_size= len(fd),\n",
        "    early_stop_after=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRFPSXulftw8"
      },
      "source": [
        "model = FoodClassificationCnnModel(config[\"no_of_classes\"],device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the dataloaders for train and test Dataset"
      ],
      "metadata": {
        "id": "B71YY6DVTizC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gq-XTF0Uht5"
      },
      "source": [
        "dataloader = DataLoader(fd,batch_size=config[\"batch_size\"], \n",
        "                        collate_fn=collate_batch,pin_memory=True, \n",
        "                        shuffle=True,\n",
        "                        num_workers=config[\"num_workers\"])\n",
        "dataloader_test = DataLoader(fd_test,batch_size=config[\"batch_size\"], \n",
        "                             collate_fn=collate_batch, pin_memory=True,\n",
        "                             num_workers=config[\"num_workers\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "assert len(dataloader.dataset) > len(dataloader_test.dataset)\n",
        "stopping_func = early_stopping(config[\"early_stop_after\"])\n",
        "loss_fn = config[\"loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSMR0lhZ-sa"
      },
      "source": [
        "assert len(dataloader.dataset) > len(dataloader_test.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqG5nzqkMjM2"
      },
      "source": [
        "wandb.login()\n",
        "run = wandb.init(project=\"food-classification\", entity=\"gege-hoho\", config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop:\n",
        "\n",
        "1.   get food item name and label from dataset\n",
        "2.   forward it to the model\n",
        "3.   compare ground truth with prediction\n",
        "4.   calculate loss\n",
        "5.   feed loss to model\n",
        "6.   log acurracy to wandb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pNz7n9u5Tou0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qLYcvY8Uket"
      },
      "source": [
        "print(f\"Train with LR:{config['lr']}\")\n",
        "assert len(dataloader.dataset) > len(dataloader_test.dataset)\n",
        "print(device)\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "  model.train()\n",
        "  x1 = time.perf_counter()\n",
        "  for data in dataloader:    \n",
        "    y_train = data[0].to(device)\n",
        "    X_train = data[1].to(device)\n",
        "    #X_train = X_train.mean(dim=1)\n",
        "    y_pred = model.forward(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    wandb.log({\"train-loss\": loss})\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  #scheduler.step()\n",
        "  x2 = time.perf_counter()\n",
        "  print(f\"Epoch Time: {x2-x1}\")\n",
        "  wandb.log({\"epoch-time\": x2-x1})\n",
        "  if epoch % 1 == 0:\n",
        "    acc_tr = model.get_accuracy(X_train,y_train)\n",
        "    print(f\"{epoch} Accuracy Train {acc_tr}\")\n",
        "    wandb.log({\"train-acc\": acc_tr})\n",
        "    acc_te = 0\n",
        "    rec_te = np.zeros((len(fd_test.cats),))\n",
        "    pre_te = np.zeros((len(fd_test.cats),))\n",
        "    model.eval()\n",
        "    for i,test in enumerate(dataloader_test):\n",
        "      y_test = test[0].to(device)\n",
        "      X_test = test[1].to(device)\n",
        "      #X_test = X_test.mean(dim=1)\n",
        "      acc_te += model.get_accuracy(X_test,y_test)\n",
        "    acc_te = acc_te/(i+1)\n",
        "    wandb.log({\"test-acc\": acc_te})\n",
        "    print(f\"{epoch} Accuracy Test {acc_te}\")\n",
        "    if stopping_func(acc_te):\n",
        "      print(\"early stopping\")\n",
        "      break\n",
        "  if epoch % 5 ==0:\n",
        "    crawled_food = test_crawled(model,fd.cats)[0:500][[\"meal_name\",\"tokens\",\"category\",\"pct\"]]\n",
        "    wandb.log({\"crawled_food\": crawled_food})\n",
        "    print(\"logged cralwed food\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the created model"
      ],
      "metadata": {
        "id": "kkijLFjWUDXs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5rHYaB2DpMX"
      },
      "source": [
        "torch.save(model,\"model93.2\")\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model again for evaluation"
      ],
      "metadata": {
        "id": "uTShs7IOUFl-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjjagd0eKR1b"
      },
      "source": [
        "model = torch.load(\"model93.2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion_matrix(y_pred,y_true,labels):\n",
        "  y_pred = y_pred.cpu()\n",
        "  y_true = y_true.cpu()\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  return cm\n",
        "  \n",
        "def get_evaluation(y_pred,y_true,labels):\n",
        "  d = [[0,0,0] for _ in range(len(labels))]\n",
        "  for y_p, y_t in zip(y_pred,y_true):\n",
        "    if y_p == y_t:\n",
        "      d[y_t][0] += 1\n",
        "    else:\n",
        "      d[y_p][2] += 1 # false negative for class y_p\n",
        "      d[y_t][1] += 1 # false positive for class y_t\n",
        "  recall = [x[0]/(x[0]+x[2]) if x[0]+x[2] != 0 else np.nan for x in d]\n",
        "  prec = [x[0]/(x[0]+x[1])if x[0]+x[1] != 0 else np.nan for x in d]\n",
        "  return np.array(recall),np.array(prec)\n",
        "\n",
        "def print_evaluation(values,labels,categories):\n",
        "  for x,l in zip(values,labels):\n",
        "    print(f\"{l}: {x:.2f}\")"
      ],
      "metadata": {
        "id": "xHBEngXxK6bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Acurracy, Recall and Precision for the Model"
      ],
      "metadata": {
        "id": "9j93mXijUK53"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDmAOVKFlJD4"
      },
      "source": [
        "model.eval()\n",
        "dataloader_eval = DataLoader(fd_test,batch_size=150000, collate_fn=collate_batch_eval, pin_memory=True)\n",
        "wrong_data =  []\n",
        "for i,test in enumerate(dataloader_eval):\n",
        "  y_test = test[0].to(device)\n",
        "  y_pred = model(test[1].to(device))\n",
        "  tokens = test[2]\n",
        "  y_pred = torch.argmax(y_pred, dim=1)\n",
        "  for y_p, y_t,tokens in zip(y_pred,y_test,tokens):\n",
        "    if y_p != y_t:\n",
        "      #print(f\"Expected {fd_test.cats[y_t]} Got {fd_test.cats[y_p]} for {tokens}\")\n",
        "      wrong_data.append((fd_test.cats[y_t],fd_test.cats[y_p],tokens)) #actual cat, predicted cat, tokens\n",
        "    #X_test = X_test.mean(dim=1)\n",
        "  \n",
        "  recall,prec = get_evaluation(y_pred,y_test, fd_test.cats)\n",
        "  cm = get_confusion_matrix(y_pred,y_test,fd_test.cats)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Recall\")\n",
        "print(f\"Mean: {np.mean(recall)}\")\n",
        "print(f\"Std: {np.std(recall)}\")\n",
        "print(f\"Min: {np.min(recall)}\")\n",
        "print(f\"Max: {np.max(recall)}\")\n",
        "print(\"Precision\")\n",
        "print(f\"Mean: {np.mean(prec)}\")\n",
        "print(f\"Std: {np.std(prec)}\")\n",
        "print(f\"Min: {np.min(prec)}\")\n",
        "print(f\"Max: {np.max(prec)}\")"
      ],
      "metadata": {
        "id": "FKuf8ojp7zov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fd_test.cats[np.argmin(recall)])\n",
        "print(fd_test.cats[np.argmax(recall)])\n",
        "print(fd_test.cats[np.argmin(prec)])\n",
        "print(fd_test.cats[np.argmax(prec)])"
      ],
      "metadata": {
        "id": "Zg6AMpqA-UeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(p,r) in enumerate(zip(prec, recall)):\n",
        "  print(f\"{fd_test.cats[i].lower()};{(p*100):.2f}%;{(r*100):.2f}%\")"
      ],
      "metadata": {
        "id": "O8M28giQA29S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mt0FpicKn81"
      },
      "source": [
        "TopK accuraccy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiHLd2J_JTiH"
      },
      "source": [
        "k = 3\n",
        "y_pred = model(test[1].to(device))\n",
        "y_test = test[0].to(device)\n",
        "topk = torch.topk(y_pred, k, dim=1)\n",
        "s = 0\n",
        "count = y_pred.shape[0]\n",
        "for i in range(topk.indices.shape[1]):\n",
        "  res = y_test -topk.indices[:,i]\n",
        "  s += (res == 0).sum()\n",
        "\n",
        "print(s/count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcSGTQH28_yh"
      },
      "source": [
        "Look at the created categories for MyFitnessPal data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBsBGiroGeh6"
      },
      "source": [
        "crawled_food = test_crawled(model,fd.cats)\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', 100):\n",
        "  display(crawled_food[0:400][[\"meal_name\",\"tokens\",\"category\",\"pct\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example how get a prediction for a given name"
      ],
      "metadata": {
        "id": "rCsIc2ODUV_F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1aTsO50GrFb"
      },
      "source": [
        "name = \"FRUIT WAVE\"\n",
        "x = embedd(bigram_model[fd.preprocess(name.split(\" \"))]).float().to(device)\n",
        "x = x.unsqueeze(0)\n",
        "print(x.shape)\n",
        "model.eval()\n",
        "z = model.forward(x)\n",
        "ss = torch.nn.Softmax()\n",
        "z2 = ss(z)\n",
        "fd.cats[torch.argmax(z2, dim=1)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}