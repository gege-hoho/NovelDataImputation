{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddingModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wordembedding\n",
        "This Notebook creates the wordembedding model and bigram model based on the USDA fooddatabase\n",
        "\n"
      ],
      "metadata": {
        "id": "E6EiEtoVMhNr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiCIt2VQKRn1"
      },
      "source": [
        "import os\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset from the usda"
      ],
      "metadata": {
        "id": "RsV6-72l2U_g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njCEQZa-KIAl"
      },
      "source": [
        "if os.path.exists(\"zip.zip\"):\n",
        "  assert False\n",
        "urllib.request.urlretrieve(\"https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-10-28.zip\", \"zip.zip\")\n",
        "!unzip zip.zip -d data\n",
        "!mkdir data/models\n",
        "#urllib.request.urlretrieve(\"https://github.com/andreamorgar/recipe-adaptation/raw/main/models/v3/modelo3\",\"data/models/modelo3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packets and install necessary nltk toolkit packets"
      ],
      "metadata": {
        "id": "oLCROaBz2Zyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwLYhhcHKfTj"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load usda model data into pandas"
      ],
      "metadata": {
        "id": "0L1bsv-L2eXL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YA3211JL2SG"
      },
      "source": [
        "branded_food = pd.read_csv(\"data/branded_food.csv\")\n",
        "food2 = pd.read_csv(\"data/food.csv\")\n",
        "food2 = branded_food.merge(food2, on='fdc_id')\n",
        "del branded_food\n",
        "food = pd.DataFrame(food2[\"description\"])\n",
        "food[\"branded_food_category\"] = food2[\"branded_food_category\"].astype('category')\n",
        "food[\"brand_name\"] = food2[\"brand_name\"].fillna(\"\").astype(str)\n",
        "del food2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the textual preprocessing pipeline\n",
        "\n",
        "\n",
        "*   Tokenization\n",
        "*   Stopword removal\n",
        "*   Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "cPZWHv6s2nuZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvxfILADK2gq"
      },
      "source": [
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "custom = [\"gal\", \"oz\", \"t\", \"tsp\", \"teaspoon\", \n",
        "          \"tablespoon\", \"tbl\", \"tbs\", \"tbsp\",\n",
        "          \"fl\", \"oz\", \"gil\", \"ounce\", \"ml\", \"l\",\n",
        "          \"dl\", \"lb\", \"pund\", \"mg\", \"g\", \"kg\", \"gram\", \"cup\"]\n",
        "lst_stopwords.extend(custom)\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def preprocess2(name):\n",
        "    name = re.sub(r'[^\\w\\s]', '', str(name).lower().strip())\n",
        "    name = tokenizer.tokenize(name)\n",
        "    name = [x for x in name if x not in lst_stopwords and len(x)>2]\n",
        "    name = [x for x in name if not any(char.isdigit() for char in x)] \n",
        "    name = [lemmatizer.lemmatize(x) for x in name]\n",
        "    return name\n",
        "\n",
        "def preprocess(name):\n",
        "    name = preprocess2(name)\n",
        "    name = list(set(name))\n",
        "    #todo bigrams\n",
        "    return name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert food item names into token lists\n",
        "\n",
        "And train bigram model on the token lists"
      ],
      "metadata": {
        "id": "ZNA5Ta-CLplo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqFr9l06Lv1k"
      },
      "source": [
        "lst_corpus = []\n",
        "for index, row in food.iterrows():\n",
        "  brand = row[\"brand_name\"]\n",
        "  x = str(row[\"description\"])\n",
        "  lst_brand = preprocess2(brand)\n",
        "  lst_words = preprocess2(x)\n",
        "  if not any(x in lst_words for x in lst_brand):\n",
        "    lst_words = lst_brand + lst_words\n",
        "\n",
        "  if index % 100000 == 0:\n",
        "    print(index)\n",
        "  #lst_grams = [\" \".join(lst_words[i:i+2]) for i in range(0, len(lst_words), 1)]\n",
        "  lst_corpus.append(lst_words)\n",
        "bigram_mdl = Phrases(lst_corpus, min_count=5, threshold=2, delimiter=b' ')\n",
        "bigrams = bigram_mdl[lst_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save bigram model to file"
      ],
      "metadata": {
        "id": "xmHGxbLLL2vb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XMLBZTsPU-A"
      },
      "source": [
        "bigram_mdl.save(\"data/models/bigram_model.pkl\")\n",
        "all_sentences = list(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeNsSwvXT_bC"
      },
      "source": [
        "bigram_mdl[\"red\",\"bull\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train word2vec model on word corpus and save to file"
      ],
      "metadata": {
        "id": "PHgFtvQyL5Vv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f99qDUXvI9dM"
      },
      "source": [
        "model = Word2Vec(all_sentences, min_count=3, size=300, workers=4, window=5, iter=30)\n",
        "model.save(\"data/models/mymodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now one just have to download the models to use in FoodClassification Notebook or in classifer.py\n"
      ],
      "metadata": {
        "id": "wrT566N4MPmR"
      }
    }
  ]
}